{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn contains loss functions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__python version: 3.5.2 (default, Oct  7 2020, 17:19:02) \n",
      "[GCC 5.4.0 20160609]\n",
      "__torch version: 1.5.1\n",
      "__cuda is available: True\n",
      "__cuda version : 10.2\n",
      "__number cuda devices: 1\n",
      "__available devices  1\n",
      "__current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#print python version\n",
    "print('__python version:', sys.version)\n",
    "\n",
    "# print torch version\n",
    "print('__torch version:', torch.__version__)\n",
    "\n",
    "# print if cuda is available (True or False)\n",
    "print('__cuda is available:', torch.cuda.is_available())\n",
    "\n",
    "# print cuda informations\n",
    "print('__cuda version :', torch.version.cuda)\n",
    "print('__number cuda devices:', torch.cuda.device_count())\n",
    "print ('__available devices ', torch.cuda.device_count())\n",
    "print ('__current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-0.2847, -1.8747,  1.2070, -0.4012,  0.6107],\n",
      "        [ 1.5438,  1.4165, -0.3873, -1.0945, -0.4519],\n",
      "        [ 1.7299, -0.5388, -0.3043,  1.0659,  0.8976]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "target:  tensor([[-0.2769, -0.2305,  0.0025, -0.6237,  2.0353],\n",
      "        [ 1.2126,  0.4152, -0.4625, -0.6712, -0.6937],\n",
      "        [-0.9768, -0.1945,  1.5002, -0.9801,  0.9110]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "my_device = torch.device(\"cuda\") # can be also \"cpu\"\n",
    "my_dtype = torch.float\n",
    "\n",
    "# Create random Tensors\n",
    "\n",
    "# This returns a tensor of size 3 × 5\n",
    "# filled with values from standard normal distribution, that is mean is 0 and variance is 1.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients during the backward pass.\n",
    "input = torch.randn(3, 5, device=my_device, dtype=my_dtype, requires_grad=True) \n",
    "\n",
    "target = torch.randn(3, 5, device=my_device, dtype=my_dtype)\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size=torch.Size([3, 5]), target size=torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Get size of tensors input and target\n",
    "print(\"input size={}, target size={}\".format(input.shape,target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad\n",
    "# If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x \n",
    "# with respect to some scalar value.\n",
    "# No value actually !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value :  tensor(0.8994, device='cuda:0', grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "mae_loss = nn.L1Loss() # Define a loss function : The Mean Absolute Error (MAE)\n",
    "loss = mae_loss(input, target) # Apply the loss function to input and target\n",
    "print('loss value : ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. \n",
    "These are accumulated into x.grad for every parameter x. In pseudo-code: x.grad += dloss/dx\n",
    "\n",
    "optimizer.step updates the value of x using the gradient x.grad. \n",
    "For example, the SGD optimizer performs:\n",
    "(lr = learning rate)\n",
    "\n",
    "x += -lr * x.grad\n",
    "\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. \n",
    "\n",
    "It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "\n",
    "If you have multiple losses (loss1, loss2) you can sum them and then call backwards once:\n",
    "\n",
    "loss3 = loss1 + loss2\n",
    "loss3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # compute the gradient of input tensor (the only one with requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0667, -0.0667,  0.0667,  0.0667, -0.0667],\n",
       "        [ 0.0667,  0.0667,  0.0667, -0.0667,  0.0667],\n",
       "        [ 0.0667, -0.0667, -0.0667,  0.0667, -0.0667]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get gradient of tensor input saved in a field named grad.\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad.zero_() # Manually zero the input gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[-0.3129],\n",
      "        [ 1.5263]]), A=tensor([[-1.8611]])\n",
      "torch.matmul(x,A)=tensor([[ 0.5824],\n",
      "        [-2.8406]]), nb elem=2\n",
      "b=tensor([-0.1281])\n",
      "torch.matmul(x,A)+b=tensor([[ 0.4543],\n",
      "        [-2.9687]])\n"
     ]
    }
   ],
   "source": [
    "# Just to verify torch matmul properties\n",
    "# see : The difference between torch.mul, torch.mm, torch.bmm, torch.matmul\n",
    "# https://www.programmersought.com/article/80074664295/\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randn(2, 1) # torch.Size([2, 1])\n",
    "A = torch.randn(1, 1) # torch.Size([1, 1])\n",
    "b = torch.randn(1) # torch.Size([1])\n",
    "\n",
    "# torch matmul -> If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "result = torch.matmul(x,A) # torch.Size([2, 1])\n",
    "\n",
    "print(\"x={}, A={}\".format(x,A))\n",
    "print(\"torch.matmul(x,A)={}, nb elem={}\".format(result,result.numel()))\n",
    "print(\"b={}\".format(b))\n",
    "result2 = result + b # torch.Size([2, 1])\n",
    "print(\"torch.matmul(x,A)+b={}\".format(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values of A=tensor([[-0.4984]], requires_grad=True), b=tensor([-0.1915], requires_grad=True)\n",
      "Learning rate=0.1\n",
      "--------------------------------------------------\n",
      "0 -> loss=2.16462779045105\n",
      "A.grad=tensor([[-2.8777]]),b.grad=tensor([-0.0906])\n",
      "A=tensor([[-0.2106]], requires_grad=True), b=tensor([-0.1825], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "1 -> loss=1.4158093929290771\n",
      "A.grad=tensor([[-2.3196]]),b.grad=tensor([-0.1286])\n",
      "A=tensor([[0.0214]], requires_grad=True), b=tensor([-0.1696], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "2 -> loss=0.9280121326446533\n",
      "A.grad=tensor([[-1.8709]]),b.grad=tensor([-0.1482])\n",
      "A=tensor([[0.2084]], requires_grad=True), b=tensor([-0.1548], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "3 -> loss=0.6095335483551025\n",
      "A.grad=tensor([[-1.5098]]),b.grad=tensor([-0.1551])\n",
      "A=tensor([[0.3594]], requires_grad=True), b=tensor([-0.1393], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "4 -> loss=0.40114569664001465\n",
      "A.grad=tensor([[-1.2191]]),b.grad=tensor([-0.1535])\n",
      "A=tensor([[0.4813]], requires_grad=True), b=tensor([-0.1239], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "5 -> loss=0.26450303196907043\n",
      "A.grad=tensor([[-0.9849]]),b.grad=tensor([-0.1466])\n",
      "A=tensor([[0.5798]], requires_grad=True), b=tensor([-0.1093], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "6 -> loss=0.17472104728221893\n",
      "A.grad=tensor([[-0.7961]]),b.grad=tensor([-0.1365])\n",
      "A=tensor([[0.6594]], requires_grad=True), b=tensor([-0.0956], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "7 -> loss=0.11561305075883865\n",
      "A.grad=tensor([[-0.6439]]),b.grad=tensor([-0.1247])\n",
      "A=tensor([[0.7238]], requires_grad=True), b=tensor([-0.0831], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "8 -> loss=0.07662598788738251\n",
      "A.grad=tensor([[-0.5211]]),b.grad=tensor([-0.1124])\n",
      "A=tensor([[0.7759]], requires_grad=True), b=tensor([-0.0719], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "9 -> loss=0.050864338874816895\n",
      "A.grad=tensor([[-0.4219]]),b.grad=tensor([-0.1001])\n",
      "A=tensor([[0.8181]], requires_grad=True), b=tensor([-0.0619], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "10 -> loss=0.03381267189979553\n",
      "A.grad=tensor([[-0.3418]]),b.grad=tensor([-0.0883])\n",
      "A=tensor([[0.8523]], requires_grad=True), b=tensor([-0.0531], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "11 -> loss=0.022507939487695694\n",
      "A.grad=tensor([[-0.2770]]),b.grad=tensor([-0.0773])\n",
      "A=tensor([[0.8800]], requires_grad=True), b=tensor([-0.0453], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "12 -> loss=0.015001827850937843\n",
      "A.grad=tensor([[-0.2246]]),b.grad=tensor([-0.0672])\n",
      "A=tensor([[0.9025]], requires_grad=True), b=tensor([-0.0386], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "13 -> loss=0.010010790079832077\n",
      "A.grad=tensor([[-0.1822]]),b.grad=tensor([-0.0582])\n",
      "A=tensor([[0.9207]], requires_grad=True), b=tensor([-0.0328], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "14 -> loss=0.006687639746814966\n",
      "A.grad=tensor([[-0.1479]]),b.grad=tensor([-0.0501])\n",
      "A=tensor([[0.9355]], requires_grad=True), b=tensor([-0.0278], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "15 -> loss=0.004472222179174423\n",
      "A.grad=tensor([[-0.1201]]),b.grad=tensor([-0.0430])\n",
      "A=tensor([[0.9475]], requires_grad=True), b=tensor([-0.0235], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "16 -> loss=0.0029935576021671295\n",
      "A.grad=tensor([[-0.0976]]),b.grad=tensor([-0.0367])\n",
      "A=tensor([[0.9572]], requires_grad=True), b=tensor([-0.0198], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "17 -> loss=0.0020055563654750586\n",
      "A.grad=tensor([[-0.0793]]),b.grad=tensor([-0.0313])\n",
      "A=tensor([[0.9652]], requires_grad=True), b=tensor([-0.0167], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "18 -> loss=0.0013447352685034275\n",
      "A.grad=tensor([[-0.0645]]),b.grad=tensor([-0.0266])\n",
      "A=tensor([[0.9716]], requires_grad=True), b=tensor([-0.0140], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "19 -> loss=0.0009023280581459403\n",
      "A.grad=tensor([[-0.0525]]),b.grad=tensor([-0.0225])\n",
      "A=tensor([[0.9769]], requires_grad=True), b=tensor([-0.0118], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# A minimal example of SGD (Stochastic Gradient Descent)\n",
    "# without a Neural Network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "N = 64 # amount of values inside the tensors\n",
    "\n",
    "# Define 2 equivalent tensors x and y with a torch.Size([64, 1])\n",
    "x0 = torch.randn(N, 1, requires_grad=False)\n",
    "x = x0\n",
    "y = x0\n",
    "\n",
    "# Define the model parameters to optimize\n",
    "A = torch.randn(1, 1, requires_grad=True) # size 1 x 1 (weights of the model) -> torch.Size([1, 1])\n",
    "b = torch.randn(1, requires_grad=True) # Size 1 (biases of the model) -> torch.Size([1])\n",
    "\n",
    "print(\"Initial values of A={}, b={}\".format(A,b))\n",
    "\n",
    "optimizer = optim.SGD([A, b], lr=1e-1) # Define SGD with [A, b] as parameters and learning rate to 1e-1\n",
    "\n",
    "print(\"Learning rate={}\".format(1e-1))\n",
    "\n",
    "for t in range(20):\n",
    "    print('-' * 50)\n",
    "    optimizer.zero_grad() # Clean gradients of A and b (because they have requires_grad to True)\n",
    "    # torch matmul -> If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "    y_pred = torch.matmul(x, A) + b # Calculate a noisy predicted value -> (x * A) + b\n",
    "    loss = ((y_pred - y) ** 2).mean() # Calculate loss value (y is the actual value)\n",
    "    print(\"{} -> loss={}\".format(t, loss.item())) # print loss value, need item() function to get it\n",
    "    loss.backward() # Compute gradient of A and b with loss value\n",
    "    print(\"A.grad={},b.grad={}\".format(A.grad,b.grad))\n",
    "    optimizer.step() # Update values of A and b by taking account of their gradient and the learning rate\n",
    "    # A += -lr * A.grad  ; b += -lr * b.grad\n",
    "    print(\"A={}, b={}\".format(A,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=18.0\n",
      "x grad=tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# A simple test of gradient\n",
    "# To proof the result of gradient\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True) # size 2 x 2 with all values equal to 1\n",
    "y = x + 2  # size 2 x 2 with all values equal to 3\n",
    "z = y * y * 2 # size 2 x 2 with all values equal to 18\n",
    "\n",
    "out = z.mean() # mean is equal to 18 ! (18*4/4)\n",
    "print(\"mean={}\".format(out))\n",
    "\n",
    "out.backward() # compute gradient of x\n",
    "\n",
    "# Math demonstration :\n",
    "# out = 1/4 sum of z(j) (for j from 1 to 4)\n",
    "# d.out/d.x(i) = 1/4 sum(j)[d.z(j)/d.x(i)]  ==> partial derivative of dout with respect to x (i and j from 1 to 4)\n",
    "# = 1/4 sum(j)[d.2*y(j)*y(j)/d.x(i)]\n",
    "# = 1/4 sum(j)[ 4*y(j)*(d.y(j)/d.x(i))]\n",
    "# = 1/4 sum(j)[4*(x(j)+2)*(d.(x(j)+2)/d.x(i))]\n",
    "# = sum(j)[(x(j)+2)*(d.(x(j)+2)/d.x(i))]\n",
    "# = x(i)+2    -> d.x(j)/d.x(i) = 0 if i not equal to j\n",
    "# = 3         -> for x(i) = 1  => tensor x : size 2 x 2 with all 1 values\n",
    "\n",
    "print(\"x grad={}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  weight\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "param.shape:  torch.Size([1, 1])\n",
      "param.requires_grad:  True\n",
      "=====\n",
      "name:  bias\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "param.shape:  torch.Size([1])\n",
      "param.requires_grad:  True\n",
      "=====\n",
      "model weight=Parameter containing:\n",
      "tensor([[-0.9948]], requires_grad=True), model bias=Parameter containing:\n",
      "tensor([0.7632], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "error = 22.306852340698242\n",
      "learned weight = -0.5600687861442566\n",
      "learned bias = 0.9834200143814087\n",
      "--------------------------------------------------\n",
      "error = 17.810943603515625\n",
      "learned weight = -0.17105355858802795\n",
      "learned bias = 1.1788705587387085\n",
      "--------------------------------------------------\n",
      "error = 14.222236633300781\n",
      "learned weight = 0.17709621787071228\n",
      "learned bias = 1.3523228168487549\n",
      "--------------------------------------------------\n",
      "error = 11.35754680633545\n",
      "learned weight = 0.4886828064918518\n",
      "learned bias = 1.5062344074249268\n",
      "--------------------------------------------------\n",
      "error = 9.070694923400879\n",
      "learned weight = 0.7675543427467346\n",
      "learned bias = 1.6427899599075317\n",
      "--------------------------------------------------\n",
      "error = 7.24504280090332\n",
      "learned weight = 1.0171533823013306\n",
      "learned bias = 1.763931393623352\n",
      "--------------------------------------------------\n",
      "error = 5.787503242492676\n",
      "learned weight = 1.2405593395233154\n",
      "learned bias = 1.8713847398757935\n",
      "--------------------------------------------------\n",
      "error = 4.623795986175537\n",
      "learned weight = 1.440527081489563\n",
      "learned bias = 1.9666838645935059\n",
      "--------------------------------------------------\n",
      "error = 3.6946401596069336\n",
      "learned weight = 1.619521141052246\n",
      "learned bias = 2.051192045211792\n",
      "--------------------------------------------------\n",
      "error = 2.9527199268341064\n",
      "learned weight = 1.779746413230896\n",
      "learned bias = 2.1261208057403564\n",
      "--------------------------------------------------\n",
      "error = 2.36027455329895\n",
      "learned weight = 1.9231754541397095\n",
      "learned bias = 2.1925461292266846\n",
      "--------------------------------------------------\n",
      "error = 1.8871628046035767\n",
      "learned weight = 2.0515730381011963\n",
      "learned bias = 2.251424551010132\n",
      "--------------------------------------------------\n",
      "error = 1.5093263387680054\n",
      "learned weight = 2.166518211364746\n",
      "learned bias = 2.303605556488037\n",
      "--------------------------------------------------\n",
      "error = 1.2075616121292114\n",
      "learned weight = 2.269423723220825\n",
      "learned bias = 2.3498435020446777\n",
      "--------------------------------------------------\n",
      "error = 0.9665383100509644\n",
      "learned weight = 2.36155366897583\n",
      "learned bias = 2.3908088207244873\n",
      "--------------------------------------------------\n",
      "error = 0.7740182280540466\n",
      "learned weight = 2.4440391063690186\n",
      "learned bias = 2.4270966053009033\n",
      "--------------------------------------------------\n",
      "error = 0.6202314496040344\n",
      "learned weight = 2.5178921222686768\n",
      "learned bias = 2.459235429763794\n",
      "--------------------------------------------------\n",
      "error = 0.4973771572113037\n",
      "learned weight = 2.5840182304382324\n",
      "learned bias = 2.487694501876831\n",
      "--------------------------------------------------\n",
      "error = 0.39922741055488586\n",
      "learned weight = 2.643228054046631\n",
      "learned bias = 2.5128908157348633\n",
      "--------------------------------------------------\n",
      "error = 0.3208090662956238\n",
      "learned weight = 2.69624662399292\n",
      "learned bias = 2.535194158554077\n",
      "--------------------------------------------------\n",
      "error = 0.25815126299858093\n",
      "learned weight = 2.743723154067993\n",
      "learned bias = 2.5549328327178955\n",
      "--------------------------------------------------\n",
      "error = 0.2080828845500946\n",
      "learned weight = 2.786238193511963\n",
      "learned bias = 2.5723981857299805\n",
      "--------------------------------------------------\n",
      "error = 0.1680717021226883\n",
      "learned weight = 2.8243117332458496\n",
      "learned bias = 2.5878489017486572\n",
      "--------------------------------------------------\n",
      "error = 0.13609515130519867\n",
      "learned weight = 2.8584089279174805\n",
      "learned bias = 2.6015143394470215\n",
      "--------------------------------------------------\n",
      "error = 0.11053790152072906\n",
      "learned weight = 2.888946056365967\n",
      "learned bias = 2.613598346710205\n",
      "--------------------------------------------------\n",
      "error = 0.0901096910238266\n",
      "learned weight = 2.9162957668304443\n",
      "learned bias = 2.624281406402588\n",
      "--------------------------------------------------\n",
      "error = 0.07378003746271133\n",
      "learned weight = 2.9407918453216553\n",
      "learned bias = 2.633723735809326\n",
      "--------------------------------------------------\n",
      "error = 0.060725413262844086\n",
      "learned weight = 2.9627327919006348\n",
      "learned bias = 2.6420671939849854\n",
      "--------------------------------------------------\n",
      "error = 0.050288233906030655\n",
      "learned weight = 2.9823856353759766\n",
      "learned bias = 2.64943790435791\n",
      "--------------------------------------------------\n",
      "error = 0.04194304347038269\n",
      "learned weight = 2.9999897480010986\n",
      "learned bias = 2.655947685241699\n",
      "--------------------------------------------------\n",
      "error = 0.03526994213461876\n",
      "learned weight = 3.0157594680786133\n",
      "learned bias = 2.6616954803466797\n",
      "--------------------------------------------------\n",
      "error = 0.029933374375104904\n",
      "learned weight = 3.029886245727539\n",
      "learned bias = 2.666769027709961\n",
      "--------------------------------------------------\n",
      "error = 0.025665324181318283\n",
      "learned weight = 3.042541742324829\n",
      "learned bias = 2.671246290206909\n",
      "--------------------------------------------------\n",
      "error = 0.022251548245549202\n",
      "learned weight = 3.053879737854004\n",
      "learned bias = 2.6751959323883057\n",
      "--------------------------------------------------\n",
      "error = 0.01952076144516468\n",
      "learned weight = 3.064037799835205\n",
      "learned bias = 2.6786789894104004\n",
      "--------------------------------------------------\n",
      "error = 0.017336145043373108\n",
      "learned weight = 3.073138952255249\n",
      "learned bias = 2.6817498207092285\n",
      "--------------------------------------------------\n",
      "error = 0.015588279813528061\n",
      "learned weight = 3.0812933444976807\n",
      "learned bias = 2.6844561100006104\n",
      "--------------------------------------------------\n",
      "error = 0.014189749024808407\n",
      "learned weight = 3.088599920272827\n",
      "learned bias = 2.686840295791626\n",
      "--------------------------------------------------\n",
      "error = 0.013070599175989628\n",
      "learned weight = 3.095147132873535\n",
      "learned bias = 2.6889400482177734\n",
      "--------------------------------------------------\n",
      "error = 0.012174909934401512\n",
      "learned weight = 3.1010141372680664\n",
      "learned bias = 2.690788507461548\n",
      "--------------------------------------------------\n",
      "error = 0.011457993648946285\n",
      "learned weight = 3.106271743774414\n",
      "learned bias = 2.692415237426758\n",
      "--------------------------------------------------\n",
      "error = 0.010884118266403675\n",
      "learned weight = 3.110983371734619\n",
      "learned bias = 2.6938459873199463\n",
      "--------------------------------------------------\n",
      "error = 0.010424706153571606\n",
      "learned weight = 3.115206003189087\n",
      "learned bias = 2.6951041221618652\n",
      "--------------------------------------------------\n",
      "error = 0.010056853294372559\n",
      "learned weight = 3.118990421295166\n",
      "learned bias = 2.696209669113159\n",
      "--------------------------------------------------\n",
      "error = 0.00976229552179575\n",
      "learned weight = 3.122382402420044\n",
      "learned bias = 2.69718074798584\n",
      "--------------------------------------------------\n",
      "error = 0.009526415728032589\n",
      "learned weight = 3.125422716140747\n",
      "learned bias = 2.698033332824707\n",
      "--------------------------------------------------\n",
      "error = 0.009337467141449451\n",
      "learned weight = 3.128147840499878\n",
      "learned bias = 2.6987814903259277\n",
      "--------------------------------------------------\n",
      "error = 0.009186134673655033\n",
      "learned weight = 3.1305906772613525\n",
      "learned bias = 2.6994378566741943\n",
      "--------------------------------------------------\n",
      "error = 0.009064885787665844\n",
      "learned weight = 3.1327805519104004\n",
      "learned bias = 2.7000131607055664\n",
      "--------------------------------------------------\n",
      "error = 0.008967739529907703\n",
      "learned weight = 3.1347436904907227\n",
      "learned bias = 2.700516939163208\n"
     ]
    }
   ],
   "source": [
    "# A minimal example of SGD (Stochastic Gradient Descent)\n",
    "# with a Neural Network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchsummary import summary  # pip install torchsummary\n",
    "\n",
    "# Let's make some data for a linear regression.\n",
    "A = 3.1415926\n",
    "B = 2.7189351\n",
    "error = 0.1\n",
    "N = 100 # number of data points\n",
    "\n",
    "# Data\n",
    "input_data = torch.randn(N, 1)\n",
    "\n",
    "# (noisy) Target values that we want to learn.\n",
    "target = A * input_data + B + (torch.randn(N, 1) * error)\n",
    "\n",
    "# Creating a model, making the optimizer, defining loss\n",
    "model = nn.Linear(1, 1) \n",
    "# create a Neural Network Linear with in_features=1 and out_features=1 (nn.Linear(in_features,ou_features))\n",
    "# Applies a linear transformation to the incoming x data: y = x*W^T + b\n",
    "# Note that the weights W have shape (out_features, in_features) and biases b have shape (out_features).\n",
    "\n",
    "# Verify model tensors parameters :\n",
    "for name, param in model.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print('=====')\n",
    "\n",
    "# name:  weight\n",
    "# <class 'torch.nn.parameter.Parameter'>\n",
    "# param.shape:  torch.Size([1, 1])\n",
    "# param.requires_grad:  True\n",
    "# =====\n",
    "# name:  bias\n",
    "# <class 'torch.nn.parameter.Parameter'>\n",
    "# param.shape:  torch.Size([1])\n",
    "#param.requires_grad:  True\n",
    "# =====\n",
    "\n",
    "print(\"model weight={}, model bias={}\".format(model.weight,model.bias))\n",
    "\n",
    "#model_summary(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "loss_fn = nn.MSELoss()  # the mean squared error (squared L2 norm)\n",
    "\n",
    "# Run training\n",
    "nb_iter = 50\n",
    "for _ in range(0, nb_iter):\n",
    "    optimizer.zero_grad() # Clear gradient values of model parameters\n",
    "    predictions = model(input_data)\n",
    "    # forward input_data into the model\n",
    "    # So multiply input_data to weight tensor parameter inside model\n",
    "    # Sizes are : input_data -> torch.Size([100, 1]) ; weight -> torch.Size([1, 1])\n",
    "    # torch.matmul(input_data,weight) -> torch.Size([100, 1])\n",
    "    # If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), \n",
    "    # then a batched matrix multiply is returned\n",
    "    # Then add bias tensor of size : torch.Size([1])\n",
    "    # and get a prediction -> torch.Size([100, 1])\n",
    "    loss = loss_fn(predictions, target) # calculate loss value\n",
    "    loss.backward() # calculate gradient values\n",
    "    optimizer.step() # update model parameters with gradient values\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"error = {}\".format(loss.item()))\n",
    "    print(\"learned weight = {}\".format(list(model.parameters())[0].data[0, 0]))\n",
    "    print(\"learned bias = {}\".format(list(model.parameters())[1].data[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  weight\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "param.shape:  torch.Size([14, 8])\n",
      "param.requires_grad:  True\n",
      "=====\n",
      "name:  bias\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "param.shape:  torch.Size([14])\n",
      "param.requires_grad:  True\n",
      "=====\n",
      "Output size: torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "# An other test with nn.Linear, to verify sizes\n",
    "\n",
    "input_size = 8\n",
    "output_size = 14\n",
    "batch_size = 64\n",
    "\n",
    "input = torch.FloatTensor(batch_size, input_size) # torch.Size([64, 8])\n",
    "\n",
    "net = nn.Linear(input_size, output_size) # in_features=8, out_features=14\n",
    "# net parameters -> W (weight) shape is (out_features, in_features) and b (biases) shape is (out_features)\n",
    "# W=(14x8), W^T=(8x14), b=(14), input(64x8)\n",
    "# output = input*W^T + b\n",
    "# output=(64x14)\n",
    "# rmq: operator + add tensor b to each line of tensor input*W^T\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print('=====')\n",
    "\n",
    "# name:  weight\n",
    "# <class 'torch.nn.parameter.Parameter'>\n",
    "# param.shape:  torch.Size([14, 8])\n",
    "# param.requires_grad:  True\n",
    "# =====\n",
    "# name:  bias\n",
    "# <class 'torch.nn.parameter.Parameter'>\n",
    "# param.shape:  torch.Size([14])\n",
    "# param.requires_grad:  True\n",
    "\n",
    "output = net(input)  # forward input into net\n",
    "\n",
    "print(\"Output size:\", output.size()) # torch.Size([64, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6247, -1.4768,  0.1176,  2.4309,  0.8171],\n",
      "        [ 0.7873, -1.3760, -0.4580,  0.4971, -1.5577],\n",
      "        [ 0.4515, -1.6608,  1.9961,  0.9986,  0.7098],\n",
      "        [ 0.6303, -1.0195,  0.2721, -0.5018,  0.5399]])\n",
      "tensor([ 3.0344, -0.0608,  0.2766, -0.6585,  0.1233])\n",
      "tensor([[ 3.6592, -1.5376,  0.3942,  1.7723,  0.9404],\n",
      "        [ 3.8217, -1.4368, -0.1814, -0.1615, -1.4344],\n",
      "        [ 3.4859, -1.7217,  2.2727,  0.3400,  0.8331],\n",
      "        [ 3.6647, -1.0804,  0.5488, -1.1603,  0.6632]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(4, 5)\n",
    "tensor2 = torch.randn(5)\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "r = tensor1 + tensor2\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
