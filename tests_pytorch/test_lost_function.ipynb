{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# torch.nn contains loss functions\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-0.9320,  1.6002,  0.3595,  1.8899, -0.1184],\n",
      "        [-0.8379,  0.0553,  0.8553, -0.3518,  0.8582],\n",
      "        [-0.7784, -0.9570, -1.0201,  1.6142,  2.0713]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "target:  tensor([[-2.8785e-01,  3.5644e-04,  1.3720e-01,  1.8768e+00,  4.2386e-01],\n",
      "        [ 9.7907e-02,  1.8768e+00, -3.8424e-01,  1.6855e+00,  1.0096e+00],\n",
      "        [ 6.5938e-01, -1.1361e-01, -2.6892e-01, -1.4951e+00,  6.3044e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "my_device = torch.device(\"cuda\") # can be also \"cpu\"\n",
    "my_dtype = torch.float\n",
    "\n",
    "# Create random Tensors\n",
    "\n",
    "# This returns a tensor of size 3 × 5\n",
    "# filled with values from standard normal distribution, that is mean is 0 and variance is 1.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients during the backward pass.\n",
    "input = torch.randn(3, 5, device=my_device, dtype=my_dtype, requires_grad=True) \n",
    "\n",
    "target = torch.randn(3, 5, device=my_device, dtype=my_dtype)\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size=torch.Size([3, 5]), target size=torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Get size of tensors input and target\n",
    "print(\"input size={}, target size={}\".format(input.shape,target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad\n",
    "# If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x \n",
    "# with respect to some scalar value.\n",
    "# No value actually !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value :  tensor(1.1193, device='cuda:0', grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "mae_loss = nn.L1Loss() # Define a loss function : The Mean Absolute Error (MAE)\n",
    "loss = mae_loss(input, target) # Apply the loss function to input and target\n",
    "print('loss value : ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. \n",
    "These are accumulated into x.grad for every parameter x. In pseudo-code: x.grad += dloss/dx\n",
    "\n",
    "optimizer.step updates the value of x using the gradient x.grad. \n",
    "For example, the SGD optimizer performs:\n",
    "(lr = learning rate)\n",
    "\n",
    "x += -lr * x.grad\n",
    "\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. \n",
    "\n",
    "It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "\n",
    "If you have multiple losses (loss1, loss2) you can sum them and then call backwards once:\n",
    "\n",
    "loss3 = loss1 + loss2\n",
    "loss3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0667,  0.0667,  0.0667,  0.0667, -0.0667],\n",
       "        [-0.0667, -0.0667,  0.0667, -0.0667, -0.0667],\n",
       "        [-0.0667, -0.0667, -0.0667,  0.0667,  0.0667]], device='cuda:0')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get gradient of tensor input\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad.zero_() # Manually zero the input gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "0 -> 1.555985927581787\n",
      "A=tensor([[0.7793]], requires_grad=True), b=tensor([0.8546], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "1 -> 0.8872073888778687\n",
      "A=tensor([[0.8757]], requires_grad=True), b=tensor([0.6718], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "2 -> 0.513855516910553\n",
      "A=tensor([[0.9402]], requires_grad=True), b=tensor([0.5308], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "3 -> 0.30288925766944885\n",
      "A=tensor([[0.9823]], requires_grad=True), b=tensor([0.4214], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "4 -> 0.1819668561220169\n",
      "A=tensor([[1.0090]], requires_grad=True), b=tensor([0.3362], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "5 -> 0.11151330918073654\n",
      "A=tensor([[1.0249]], requires_grad=True), b=tensor([0.2694], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "6 -> 0.069713294506073\n",
      "A=tensor([[1.0337]], requires_grad=True), b=tensor([0.2169], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "7 -> 0.04442751780152321\n",
      "A=tensor([[1.0376]], requires_grad=True), b=tensor([0.1753], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "8 -> 0.028822962194681168\n",
      "A=tensor([[1.0384]], requires_grad=True), b=tensor([0.1423], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "9 -> 0.019000714644789696\n",
      "A=tensor([[1.0372]], requires_grad=True), b=tensor([0.1159], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "10 -> 0.012700756080448627\n",
      "A=tensor([[1.0349]], requires_grad=True), b=tensor([0.0947], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "11 -> 0.008589709177613258\n",
      "A=tensor([[1.0320]], requires_grad=True), b=tensor([0.0776], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "12 -> 0.005865766666829586\n",
      "A=tensor([[1.0288]], requires_grad=True), b=tensor([0.0638], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "13 -> 0.004037060309201479\n",
      "A=tensor([[1.0256]], requires_grad=True), b=tensor([0.0526], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "14 -> 0.0027958154678344727\n",
      "A=tensor([[1.0226]], requires_grad=True), b=tensor([0.0434], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "15 -> 0.0019457036396488547\n",
      "A=tensor([[1.0197]], requires_grad=True), b=tensor([0.0360], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "16 -> 0.001359247136861086\n",
      "A=tensor([[1.0171]], requires_grad=True), b=tensor([0.0298], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "17 -> 0.0009523541666567326\n",
      "A=tensor([[1.0148]], requires_grad=True), b=tensor([0.0248], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "18 -> 0.000668773369397968\n",
      "A=tensor([[1.0127]], requires_grad=True), b=tensor([0.0206], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "19 -> 0.00047044639359228313\n",
      "A=tensor([[1.0109]], requires_grad=True), b=tensor([0.0172], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# A minimal example of SGD (Stochastic Gradient Descent)\n",
    "# without a Neural Network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "N = 64 # amount of values inside the tensors\n",
    "\n",
    "# Define 2 equivalent tensors x and y\n",
    "x0 = torch.randn(N, 1, requires_grad=False)\n",
    "x = x0\n",
    "y = x0\n",
    "\n",
    "#print(\"x={}, y={}\".format(x,y))\n",
    "\n",
    "# Define the model parameters to optimize\n",
    "A = torch.randn(1, 1, requires_grad=True) # size 1 x 1 (weights of the model) -> torch.Size([1, 1])\n",
    "b = torch.randn(1, requires_grad=True) # Size 1 (biases of the model) -> torch.Size([1])\n",
    "\n",
    "optimizer = optim.SGD([A, b], lr=1e-1) # Define SGD with [A, b] as parameters and learning rate to 1e-1\n",
    "\n",
    "for t in range(20):\n",
    "    print('-' * 50)\n",
    "    optimizer.zero_grad() # Clean gradients of A and b (because they have requires_grad to True)\n",
    "    y_pred = torch.matmul(x, A) + b # Calculate a noisy predicted value -> (x * A) + b\n",
    "    loss = ((y_pred - y) ** 2).mean() # Calculate loss value (y is the actual value)\n",
    "    print(\"{} -> {}\".format(t, loss.item())) # print loss value, need item() function to get it\n",
    "    loss.backward() # Compute gradient of A and b with loss value\n",
    "    optimizer.step() # Update values of A and b by taking account of their gradient and the learning rate\n",
    "    print(\"A={}, b={}\".format(A,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=18.0\n",
      "x grad=tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# A simple test of gradient\n",
    "# to proof the result of gradient\n",
    "x = torch.ones(2, 2, requires_grad=True) # size 2 x 2 with all 1 values\n",
    "y = x + 2  # size 2 x 2 with all 3 values\n",
    "z = y * y * 2 # size 2 x 2 with all 18 values\n",
    "\n",
    "out = z.mean() # mean is equal to 18 ! (18*4/4)\n",
    "print(\"mean={}\".format(out))\n",
    "\n",
    "out.backward() # compute gradient of x\n",
    "# out = 1/4 sum of z(j) (for j from 1 to 4)\n",
    "# d.out/d.x(i) = 1/4 sum(j)[d.z(j)/d.x(i)]  ==> partial derivative of dout with respect to x (i and j from 1 to 4)\n",
    "# = 1/4 sum(j)[d.2*y(j)*y(j)/d.x(i)]\n",
    "# = 1/4 sum(j)[ 4*y(j)*(d.y(j)/d.x(i))]\n",
    "# = 1/4 sum(j)[4*(x(j)+2)*(d.(x(j)+2)/d.x(i))]\n",
    "# = sum(j)[(x(j)+2)*(d.(x(j)+2)/d.x(i))]\n",
    "# = x(i)+2    -> d.x(j)/d.x(i) = 0 if i not equal to j\n",
    "# = 3 -> for x(i) = 1  => tensor x : size 2 x 2 with all 1 values\n",
    "print(\"x grad={}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=torch.Size([64, 1])\n",
      "nb elem in result=64\n"
     ]
    }
   ],
   "source": [
    "# Just to verify mult property of tensors\n",
    "tensor1 = torch.randn(64, 1) # torch.Size([64, 1])\n",
    "tensor2 = torch.randn(1, 1) # torch.Size([1, 1])\n",
    "\n",
    "result = torch.matmul(tensor1, tensor2).size()  # torch.Size([64, 1])\n",
    "print(\"result={}\".format(result))\n",
    "print(\"nb elem in result={}\".format(result.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  weight\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "param.shape:  torch.Size([1, 1])\n",
      "param.requires_grad:  True\n",
      "=====\n",
      "name:  bias\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "param.shape:  torch.Size([1])\n",
      "param.requires_grad:  True\n",
      "=====\n",
      "model weight=Parameter containing:\n",
      "tensor([[0.9866]], requires_grad=True), model bias=Parameter containing:\n",
      "tensor([0.6161], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "error = 9.865514755249023\n",
      "learned weight = 1.2502708435058594\n",
      "learned bias = 0.8141689896583557\n",
      "--------------------------------------------------\n",
      "error = 7.811840057373047\n",
      "learned weight = 1.4815841913223267\n",
      "learned bias = 0.9938012361526489\n",
      "--------------------------------------------------\n",
      "error = 6.192173957824707\n",
      "learned weight = 1.6845104694366455\n",
      "learned bias = 1.1566535234451294\n",
      "--------------------------------------------------\n",
      "error = 4.913486003875732\n",
      "learned weight = 1.8625601530075073\n",
      "learned bias = 1.3042585849761963\n",
      "--------------------------------------------------\n",
      "error = 3.9029695987701416\n",
      "learned weight = 2.0188071727752686\n",
      "learned bias = 1.4380139112472534\n",
      "--------------------------------------------------\n",
      "error = 3.1035802364349365\n",
      "learned weight = 2.155942678451538\n",
      "learned bias = 1.5591930150985718\n",
      "--------------------------------------------------\n",
      "error = 2.4705801010131836\n",
      "learned weight = 2.2763237953186035\n",
      "learned bias = 1.668955683708191\n",
      "--------------------------------------------------\n",
      "error = 1.9688470363616943\n",
      "learned weight = 2.3820149898529053\n",
      "learned bias = 1.7683578729629517\n",
      "--------------------------------------------------\n",
      "error = 1.570777177810669\n",
      "learned weight = 2.4748241901397705\n",
      "learned bias = 1.8583605289459229\n",
      "--------------------------------------------------\n",
      "error = 1.2546559572219849\n",
      "learned weight = 2.556335210800171\n",
      "learned bias = 1.9398375749588013\n",
      "--------------------------------------------------\n",
      "error = 1.0033812522888184\n",
      "learned weight = 2.6279361248016357\n",
      "learned bias = 2.0135838985443115\n",
      "--------------------------------------------------\n",
      "error = 0.8034703731536865\n",
      "learned weight = 2.690842866897583\n",
      "learned bias = 2.080321788787842\n",
      "--------------------------------------------------\n",
      "error = 0.6442843079566956\n",
      "learned weight = 2.7461211681365967\n",
      "learned bias = 2.1407077312469482\n",
      "--------------------------------------------------\n",
      "error = 0.5174176692962646\n",
      "learned weight = 2.7947049140930176\n",
      "learned bias = 2.1953377723693848\n",
      "--------------------------------------------------\n",
      "error = 0.41622450947761536\n",
      "learned weight = 2.8374128341674805\n",
      "learned bias = 2.244753360748291\n",
      "--------------------------------------------------\n",
      "error = 0.3354438543319702\n",
      "learned weight = 2.874962568283081\n",
      "learned bias = 2.2894458770751953\n",
      "--------------------------------------------------\n",
      "error = 0.27090778946876526\n",
      "learned weight = 2.9079837799072266\n",
      "learned bias = 2.3298611640930176\n",
      "--------------------------------------------------\n",
      "error = 0.2193102240562439\n",
      "learned weight = 2.937027931213379\n",
      "learned bias = 2.366403818130493\n",
      "--------------------------------------------------\n",
      "error = 0.17802706360816956\n",
      "learned weight = 2.9625794887542725\n",
      "learned bias = 2.3994410037994385\n",
      "--------------------------------------------------\n",
      "error = 0.14497266709804535\n",
      "learned weight = 2.985062837600708\n",
      "learned bias = 2.429305076599121\n",
      "--------------------------------------------------\n",
      "error = 0.11848905682563782\n",
      "learned weight = 3.004850387573242\n",
      "learned bias = 2.4562976360321045\n",
      "--------------------------------------------------\n",
      "error = 0.0972561165690422\n",
      "learned weight = 3.0222690105438232\n",
      "learned bias = 2.480692148208618\n",
      "--------------------------------------------------\n",
      "error = 0.08022210747003555\n",
      "learned weight = 3.0376055240631104\n",
      "learned bias = 2.5027363300323486\n",
      "--------------------------------------------------\n",
      "error = 0.06654829531908035\n",
      "learned weight = 3.0511116981506348\n",
      "learned bias = 2.5226545333862305\n",
      "--------------------------------------------------\n",
      "error = 0.05556552857160568\n",
      "learned weight = 3.0630087852478027\n",
      "learned bias = 2.5406501293182373\n",
      "--------------------------------------------------\n",
      "error = 0.04673917219042778\n",
      "learned weight = 3.073490619659424\n",
      "learned bias = 2.5569069385528564\n",
      "--------------------------------------------------\n",
      "error = 0.03964214026927948\n",
      "learned weight = 3.0827276706695557\n",
      "learned bias = 2.571591854095459\n",
      "--------------------------------------------------\n",
      "error = 0.03393266722559929\n",
      "learned weight = 3.090869426727295\n",
      "learned bias = 2.584855318069458\n",
      "--------------------------------------------------\n",
      "error = 0.029337337240576744\n",
      "learned weight = 3.0980474948883057\n",
      "learned bias = 2.596834182739258\n",
      "--------------------------------------------------\n",
      "error = 0.025636935606598854\n",
      "learned weight = 3.104377508163452\n",
      "learned bias = 2.607651948928833\n",
      "--------------------------------------------------\n",
      "error = 0.022655878216028214\n",
      "learned weight = 3.1099607944488525\n",
      "learned bias = 2.617420196533203\n",
      "--------------------------------------------------\n",
      "error = 0.020253367722034454\n",
      "learned weight = 3.11488676071167\n",
      "learned bias = 2.6262402534484863\n",
      "--------------------------------------------------\n",
      "error = 0.01831628568470478\n",
      "learned weight = 3.1192336082458496\n",
      "learned bias = 2.6342034339904785\n",
      "--------------------------------------------------\n",
      "error = 0.01675393059849739\n",
      "learned weight = 3.123070478439331\n",
      "learned bias = 2.641392707824707\n",
      "--------------------------------------------------\n",
      "error = 0.015493304468691349\n",
      "learned weight = 3.126458168029785\n",
      "learned bias = 2.6478824615478516\n",
      "--------------------------------------------------\n",
      "error = 0.014475839212536812\n",
      "learned weight = 3.1294498443603516\n",
      "learned bias = 2.653740644454956\n",
      "--------------------------------------------------\n",
      "error = 0.013654329814016819\n",
      "learned weight = 3.1320924758911133\n",
      "learned bias = 2.6590282917022705\n",
      "--------------------------------------------------\n",
      "error = 0.012990857474505901\n",
      "learned weight = 3.134427309036255\n",
      "learned bias = 2.6638007164001465\n",
      "--------------------------------------------------\n",
      "error = 0.01245484221726656\n",
      "learned weight = 3.136490821838379\n",
      "learned bias = 2.668107748031616\n",
      "--------------------------------------------------\n",
      "error = 0.012021704576909542\n",
      "learned weight = 3.138314962387085\n",
      "learned bias = 2.671994686126709\n",
      "--------------------------------------------------\n",
      "error = 0.011671588756144047\n",
      "learned weight = 3.139927864074707\n",
      "learned bias = 2.675502300262451\n",
      "--------------------------------------------------\n",
      "error = 0.011388513259589672\n",
      "learned weight = 3.1413543224334717\n",
      "learned bias = 2.6786673069000244\n",
      "--------------------------------------------------\n",
      "error = 0.011159601621329784\n",
      "learned weight = 3.1426162719726562\n",
      "learned bias = 2.681523323059082\n",
      "--------------------------------------------------\n",
      "error = 0.01097442302852869\n",
      "learned weight = 3.143732786178589\n",
      "learned bias = 2.6841001510620117\n",
      "--------------------------------------------------\n",
      "error = 0.010824620723724365\n",
      "learned weight = 3.144721031188965\n",
      "learned bias = 2.686424970626831\n",
      "--------------------------------------------------\n",
      "error = 0.01070340070873499\n",
      "learned weight = 3.1455960273742676\n",
      "learned bias = 2.6885223388671875\n",
      "--------------------------------------------------\n",
      "error = 0.010605278424918652\n",
      "learned weight = 3.1463708877563477\n",
      "learned bias = 2.6904144287109375\n",
      "--------------------------------------------------\n",
      "error = 0.010525866411626339\n",
      "learned weight = 3.147057294845581\n",
      "learned bias = 2.6921212673187256\n",
      "--------------------------------------------------\n",
      "error = 0.010461567901074886\n",
      "learned weight = 3.147665500640869\n",
      "learned bias = 2.6936609745025635\n",
      "--------------------------------------------------\n",
      "error = 0.010409501381218433\n",
      "learned weight = 3.1482045650482178\n",
      "learned bias = 2.69504976272583\n"
     ]
    }
   ],
   "source": [
    "# A minimal example of SGD (Stochastic Gradient Descent)\n",
    "# with a Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary  # pip install torchsummary\n",
    "\n",
    "# Let's make some data for a linear regression.\n",
    "A = 3.1415926\n",
    "B = 2.7189351\n",
    "error = 0.1\n",
    "N = 100 # number of data points\n",
    "\n",
    "# Data\n",
    "input_data = torch.randn(N, 1)\n",
    "\n",
    "# (noisy) Target values that we want to learn.\n",
    "target = A * input_data + B + (torch.randn(N, 1) * error)\n",
    "\n",
    "# Creating a model, making the optimizer, defining loss\n",
    "model = nn.Linear(1, 1) # create a Neural Network Linear with in_features 1 and out_features 1\n",
    "# Note that the weights W have shape (out_features, in_features) and biases b have shape (out_features).\n",
    "\n",
    "# Print model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print('=====')\n",
    "\n",
    "# name:  weight\n",
    "# <class 'torch.nn.parameter.Parameter'>\n",
    "# param.shape:  torch.Size([1, 1])\n",
    "# param.requires_grad:  True\n",
    "# =====\n",
    "# name:  bias\n",
    "# <class 'torch.nn.parameter.Parameter'>\n",
    "# param.shape:  torch.Size([1])\n",
    "#param.requires_grad:  True\n",
    "# =====\n",
    "\n",
    "print(\"model weight={}, model bias={}\".format(model.weight,model.bias))\n",
    "\n",
    "#model_summary(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "loss_fn = nn.MSELoss()  # the mean squared error (squared L2 norm)\n",
    "\n",
    "# Run training\n",
    "nb_iter = 50\n",
    "for _ in range(0, nb_iter):\n",
    "    optimizer.zero_grad() # Clear gradient values of model parameters\n",
    "    predictions = model(input_data)  # forward input_data into the model and get a prediction -> torch.Size([100, 1])\n",
    "    #print(predictions.shape)\n",
    "    loss = loss_fn(predictions, target) # calculate loss value\n",
    "    loss.backward() # calculate gradient values\n",
    "    optimizer.step() # update model parameters with gradient values\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"error = {}\".format(loss.item()))\n",
    "    print(\"learned weight = {}\".format(list(model.parameters())[0].data[0, 0]))\n",
    "    print(\"learned bias = {}\".format(list(model.parameters())[1].data[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
