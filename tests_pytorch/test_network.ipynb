{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A REMINDER : CLASS torch.nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "Applies a linear transformation to the incoming data: y = x*W^T + b\n",
    "\n",
    "Parameters:\n",
    "\n",
    "in_features -> size of each input sample (i.e. size of x)\n",
    "\n",
    "out_features -> size of each output sample (i.e. size of y)\n",
    "\n",
    "bias -> If set to False, the layer will not learn an additive bias. Default: True\n",
    "    \n",
    "Note that the weights W have shape (out_features, in_features) and biases b have shape (out_features). \n",
    "They are initialized randomly and can be changed later \n",
    "(e.g. during the training of a Neural Network they are updated by some optimization algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete example of nn.Linear\n",
    "# Create a tensor x of size 3 x 2\n",
    "# Where x contains three inputs (i.e. the batch size is 3), x[0], x[1] and x[3], each of size 2\n",
    "x = torch.tensor([[1.0, -1.0],\n",
    "                  [0.0,  1.0],\n",
    "                  [0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 5\n",
    "\n",
    "m = nn.Linear(in_features, out_features)\n",
    "\n",
    "y = m(x)\n",
    "\n",
    "# create a fully connected linear layer, which takes input x of shape (batch_size, in_features), \n",
    "# where batch size is the number of inputs (each of size in_features) \n",
    "# which are passed to the network at once (as a single tensor), \n",
    "# and transforms it by the linear equation y = x*W^T + b into a tensor y of shape (batch_size, out_features).\n",
    "\n",
    "# Internal parameters :\n",
    "# -> Weights W have shape (out_features, in_features) -> W(5,2)\n",
    "# -> Biases b have shape (out_features) -> b(5)\n",
    "\n",
    "# Output :\n",
    "# y is going to be of shape (batch size, out_features) -> y(3, 5).\n",
    "\n",
    "# and (behind the scenes) it is computed as:\n",
    "# y = x.matmul(m.weight.t()) + m.bias  # y = x*W^T + b\n",
    "# ==> y(3,5) = x(3,2) * W^T(2,5) + b(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0619, -0.1240],\n",
      "        [ 0.3286, -0.3996],\n",
      "        [-0.1813, -0.2766],\n",
      "        [-0.4727,  0.4290],\n",
      "        [-0.7036,  0.3730]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# m.weight and m.bias were initialized randomly.\n",
    "print(m.weight.size())\n",
    "print(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "Parameter containing:\n",
      "tensor([-0.1196,  0.3142, -0.4347,  0.2551, -0.6858], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(m.bias.size())\n",
    "print(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[ 0.0663,  1.0424, -0.3395, -0.6467, -1.7625],\n",
      "        [-0.2436, -0.0855, -0.7112,  0.6841, -0.3128],\n",
      "        [-0.1196,  0.3142, -0.4347,  0.2551, -0.6858]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# The output y is :\n",
    "print(y.size())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # define a shortcut 'nn' to avoid always use 'torch.nn'\n",
    "import torch.nn.functional as F # define a shortcut 'F' to avoid always use 'torch.nn.functional' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"relu.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([ 1., -1.])\n",
      "--------------------------------\n",
      "Weight : Parameter containing:\n",
      "tensor([[ 0.2480, -0.6331],\n",
      "        [-0.4280, -0.3598],\n",
      "        [-0.6689,  0.4010]], requires_grad=True)\n",
      "Bias : Parameter containing:\n",
      "tensor([0.1330, 0.6197, 0.4933], requires_grad=True)\n",
      "--------------------------------\n",
      "output : tensor([ 1.0141,  0.5516, -0.5766], grad_fn=<AddBackward0>)\n",
      "y : tensor([ 1.0141,  0.5516, -0.5766], grad_fn=<AddBackward0>)\n",
      "--------------------------------\n",
      "activation : tensor([1.0141, 0.5516, 0.0000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example 1 :\n",
    "# =========\n",
    "# A simple network : 2 Inputs, 3 outputs and 1 relu activation Function\n",
    "# Defined step by step with a Linear Module\n",
    "\n",
    "x = torch.tensor([1.0, -1.0]) # define a tensor torch.Size([2])\n",
    "\n",
    "net = nn.Linear(2, 3) # define a Linear network with in_features=2, out_features=3, bias=True (by default)\n",
    "\n",
    "print(\"input : {}\".format(x))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"Weight : {}\".format(net.weight))\n",
    "print(\"Bias : {}\".format(net.bias))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "output = net.forward(x) # forward tensor into the net and get the output\n",
    "y = x.matmul(net.weight.t()) + net.bias # just to verify the equation : y = x*W^T + b\n",
    "\n",
    "print(\"output : {}\".format(output))\n",
    "print(\"y : {}\".format(y))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# ReLU: ReLU stands for Rectified Linear Unit. \n",
    "# It takes a real-valued input and thresholds it at zero (replaces negative values with zero)\n",
    "# f(x) = max(0, x)\n",
    "activation = F.relu(output) # compute activation of output\n",
    "\n",
    "print(\"activation : {}\".format(activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([ 1., -1.])\n",
      "--------------------------------\n",
      "network structure : Sequential(\n",
      "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "--------------------------------\n",
      "Weight : Parameter containing:\n",
      "tensor([[-0.5955, -0.6533],\n",
      "        [-0.4863,  0.3000],\n",
      "        [-0.3434,  0.2724]], requires_grad=True)\n",
      "Bias : Parameter containing:\n",
      "tensor([-0.3994, -0.2261, -0.1660], requires_grad=True)\n",
      "Activation function : ReLU()\n",
      "--------------------------------\n",
      "activation : tensor([0., 0., 0.], grad_fn=<ReluBackward0>)\n",
      "--------------------------------\n",
      "internal output : tensor([-0.3416, -1.0125, -0.7817], grad_fn=<AddBackward0>)\n",
      "ReLU(output) :tensor([0., 0., 0.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example 2 :\n",
    "# =========\n",
    "# The same network defined with a Sequential container\n",
    "# A sequential container. Modules will be added to it in the order they are passed in the constructor.\n",
    "\n",
    "# nn.ReLU() creates an nn.Module which you can be added to an nn.Sequential model.\n",
    "# nn.functional.relu is just the functional API call to the relu function.\n",
    "\n",
    "x = torch.tensor([1.0, -1.0]) # define a tensor torch.Size([2])\n",
    "\n",
    "net = nn.Sequential(\n",
    "                    nn.Linear(2, 3),\n",
    "                    nn.ReLU()\n",
    "                    );\n",
    "\n",
    "print(\"input : {}\".format(x))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"network structure : {}\".format(net))\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"Weight : {}\".format(net[0].weight))  # net parameters contained at index 0\n",
    "print(\"Bias : {}\".format(net[0].bias)) # net parameters contained at index 0\n",
    "print(\"Activation function : {}\".format(net[1]))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "activation = net.forward(x) # forward tensor into the net and get the output\n",
    "print(\"activation : {}\".format(activation))\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "output = torch.matmul(x,net[0].weight.t()) + net[0].bias;  # Just to verify\n",
    "print(\"internal output : {}\".format(output))\n",
    "print(\"ReLU(output) :{}\".format(torch.nn.ReLU().forward(output))) # Just to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([ 1., -1.])\n",
      "--------------------------------\n",
      "Weight : Parameter containing:\n",
      "tensor([[-0.1354,  0.0456],\n",
      "        [-0.5058,  0.6184],\n",
      "        [-0.5801,  0.7063]], requires_grad=True)\n",
      "Bias : Parameter containing:\n",
      "tensor([-0.6495,  0.6350,  0.2916], requires_grad=True)\n",
      "Activation function : ReLU()\n",
      "--------------------------------\n",
      "activation : tensor([0., 0., 0.], grad_fn=<ReluBackward0>)\n",
      "--------------------------------\n",
      "internal output : tensor([-0.8306, -0.4893, -0.9948], grad_fn=<AddBackward0>)\n",
      "ReLU(output) :tensor([0., 0., 0.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example 3 :\n",
    "# =========\n",
    "# The same network defined as a custom Module subclass without Sequantial container\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()  # Call parent nn.Module constructor\n",
    "        self.l1 = nn.Linear(n_inputs, n_outputs)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.act1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "model = NeuralNetwork(2, 3)\n",
    "x = torch.tensor([1.0, -1.0]) # define a tensor torch.Size([2])\n",
    "activation = model.forward(x)\n",
    "\n",
    "print(\"input : {}\".format(x))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Weight : {}\".format(model.l1.weight)) \n",
    "print(\"Bias : {}\".format(model.l1.bias))\n",
    "print(\"Activation function : {}\".format(model.act1))\n",
    "print(\"--------------------------------\")\n",
    "print(\"activation : {}\".format(activation))\n",
    "print(\"--------------------------------\")\n",
    "output = torch.matmul(x,model.l1.weight.t()) + model.l1.bias;  # Just to verify\n",
    "print(\"internal output : {}\".format(output))\n",
    "print(\"ReLU(output) :{}\".format(torch.nn.ReLU().forward(output))) # Just to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "network structure : Sequential(\n",
      "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "--------------------------------\n",
      "input : tensor([ 1., -1.])\n",
      "--------------------------------\n",
      "Weight : Parameter containing:\n",
      "tensor([[ 0.1587, -0.3943],\n",
      "        [ 0.2709,  0.5950],\n",
      "        [ 0.1807,  0.2263]], requires_grad=True)\n",
      "Bias : Parameter containing:\n",
      "tensor([ 0.5144, -0.2386, -0.3587], requires_grad=True)\n",
      "Activation function : ReLU()\n",
      "--------------------------------\n",
      "activation : tensor([1.0673, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "--------------------------------\n",
      "internal output : tensor([ 1.0673, -0.5627, -0.4043], grad_fn=<AddBackward0>)\n",
      "ReLU(output) :tensor([1.0673, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example 4 :\n",
    "# =========\n",
    "# The same network defined as a custom Module subclass with Sequantial container\n",
    "\n",
    "class NeuralNetworkSequential(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()  # Call parent nn.Module constructor\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(n_inputs, n_outputs),\n",
    "                    nn.ReLU()\n",
    "                    );\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "model = NeuralNetworkSequential(2, 3)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"network structure : {}\".format(net))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "x = torch.tensor([1.0, -1.0]) # define a tensor torch.Size([2])\n",
    "activation = model.forward(x)\n",
    "\n",
    "print(\"input : {}\".format(x))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Weight : {}\".format(model.net[0].weight)) \n",
    "print(\"Bias : {}\".format(model.net[0].bias))\n",
    "print(\"Activation function : {}\".format(model.net[1]))\n",
    "print(\"--------------------------------\")\n",
    "print(\"activation : {}\".format(activation))\n",
    "print(\"--------------------------------\")\n",
    "output = torch.matmul(x,model.net[0].weight.t()) + model.net[0].bias;  # Just to verify\n",
    "print(\"internal output : {}\".format(output))\n",
    "print(\"ReLU(output) :{}\".format(torch.nn.ReLU().forward(output))) # Just to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic unit of computation in a neural network is the neuron, often called node or unit.\n",
    "A network is built from individual nodes. Each nodes has some number of weighted inputs (weight and bias). \n",
    "These weighted inputs are summed together (a linear combination) then passed through an activation function to get the node’s output.\n",
    "\n",
    "Input Nodes :\n",
    "- The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”. \n",
    "- No computation is performed in any of the Input nodes. \n",
    "- They just pass on the information to the hidden nodes.\n",
    "\n",
    "Hidden Nodes :\n",
    "- The Hidden nodes have no direct connection with the outside world (hence the name “hidden”). \n",
    "- They perform computations and transfer information from the input nodes to the output nodes. \n",
    "- A collection of hidden nodes forms a “Hidden Layer”. \n",
    "- While a feedforward network will only have a single input layer and a single output layer, it can have zero or multiple Hidden Layers.\n",
    "\n",
    "Output Nodes :\n",
    "- The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a Feed Forward Network : 3 Layers\n",
    "# -> 2 Hidden Layers : 4 and 6 Nodes, and 1 Output Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"feed_forward_net_sigmoid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid: takes a real-valued input and squashes it to range between 0 and 1\n",
    "# σ(x) = 1 / (1 + exp(−x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sigmoid.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
