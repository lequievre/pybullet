{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum OpenAI gym environment is created for continuous control tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendulum-v0\n",
    "https://github.com/openai/gym/wiki/Pendulum-v0\n",
    "    \n",
    "The problem  is to  keep a frictionless pendulum standing up.\n",
    "\n",
    "Action :\n",
    "Box(1) -> Joint Effort between -2.0 and 2.0\n",
    "\n",
    "Starting state :\n",
    "Random angle from −π to π, and random velocity between -1 and 1\n",
    "\n",
    "Reward :\n",
    "Reward = -(theta^2 + 0.1\\*theta_dt^2 + 0.001\\*action^2)\n",
    "\n",
    "Reward is based on the angle of the pendulum (1), the angular velocity (2) of the pendulum, \n",
    "and the force applied (3).\n",
    "Agents get increased reward for keeping the pendulum (1) upright, (2) still, and (3) using little force.\n",
    "\n",
    "Theta is normalized between -pi and pi\n",
    "\n",
    "Therefore, the lowest reward is -(pi^2 + 0.1\\*8^2 + 0.001\\*2^2) = -16.2736044, \n",
    "and the highest reward is 0. \n",
    "In essence, the goal is to remain at zero angle (vertical), \n",
    "with the least rotational velocity, and the least effort.\n",
    "\n",
    "\n",
    "Episode Termination :\n",
    "<br>\n",
    "There is no specified termination into pendulum code.\n",
    "<br>\n",
    "But, Pendulum is wrapped by the TimeLimit wrapper when you call gym.make.\n",
    "<br>\n",
    "https://github.com/openai/gym/blob/master/gym/wrappers/time_limit.py\n",
    "<br>\n",
    "And step function returns true for done after 200 steps !\n",
    "<br>\n",
    "https://github.com/openai/gym/issues/1437\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.56233769, -0.82690769,  0.02425304])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Box(-2.0, 2.0, (1,), float32)\n",
      "action space low = [-2.], high = [2.]\n",
      "observation space = Box(-8.0, 8.0, (3,), float32)\n",
      "----- Shapes --------------------\n",
      "observation shape = (3,)\n",
      "action shape = (1,)\n",
      "-----------------------------------\n",
      "observation = [-0.03864939  0.99925283 -0.05990557]\n",
      "internal state = [ 1.60945535 -0.05990557]\n",
      "theta=1.609455345951143, cos(theta)=-0.03864939043110492, sin(theta)=0.9992528331805239, theta dot=-0.05990557308194777\n"
     ]
    }
   ],
   "source": [
    "print(\"action space = {}\".format(env.action_space))\n",
    "# Box(-2.0, 2.0, (1,), float32)\n",
    "# 1 float value between -2.0 and 2.0\n",
    "# Action = between -2.0 and 2.0\n",
    "print(\"action space low = {}, high = {}\".format(env.action_space.low,env.action_space.high))\n",
    "\n",
    "print(\"observation space = {}\".format(env.observation_space))\n",
    "# Box(-8.0, 8.0, (3,), float32)\n",
    "# 3 float values between -8.0 and 8.0\n",
    "# Observation/State = [cos(theta), sin(theta), theta dot]\n",
    "# index 0 -> cos(theta) [-1.0, 1.0] \n",
    "# index 1 -> sin(theta) [-1.0, 1.0]\n",
    "# index 2 -> theta dot [-8.0, 8.0]\n",
    "# The angles(thetas) are passed through the sin() and cos() function so that the observations are in the range [-1,1]. \n",
    "# This fixed range of [-1,1] helps in stabilising the training in the neural networks\n",
    "\n",
    "print(\"----- Shapes --------------------\")\n",
    "print(\"observation shape = {}\".format(env.observation_space.shape))\n",
    "print(\"action shape = {}\".format(env.action_space.shape))\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"observation = {}\".format(observation))\n",
    "print(\"internal state = {}\".format(env.state)) # env state is a numpy array [theta, theta_dot]\n",
    "print(\"theta={}, cos(theta)={}, sin(theta)={}, theta dot={}\".format(env.state[0],np.cos(env.state[0]), np.sin(env.state[0]), env.state[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "a sample of action = [1.9053117]\n",
      "[0] observation = (array([ 0.269139  , -0.96310134,  3.22683385]), -3.8767875197819475, False, {})\n",
      "[0] reward = -3.8767875197819475\n",
      "[0] internal state = [4.98488791 3.22683385]\n",
      "[0] theta=4.984887913902413, cos(theta)=0.26913900075426017, sin(theta)=-0.9631013437188208, theta dot=3.2268338527071965\n",
      "------------------------------------------------\n",
      "a sample of action = [-0.96451586]\n",
      "[1] observation = (array([ 0.39046094, -0.92061949,  2.57266584]), -2.7270282599170983, False, {})\n",
      "[1] reward = -2.7270282599170983\n",
      "[1] internal state = [5.11352121 2.57266584]\n",
      "[1] theta=5.113521206034687, cos(theta)=0.3904609413640018, sin(theta)=-0.920619494291283, theta dot=2.5726658426454745\n",
      "------------------------------------------------\n",
      "a sample of action = [1.980811]\n",
      "[2] observation = (array([ 0.46845268, -0.88348859,  1.72812736]), -2.0310301191046327, False, {})\n",
      "[2] reward = -2.0310301191046327\n",
      "[2] internal state = [5.19992757 1.72812736]\n",
      "[2] theta=5.199927573919958, cos(theta)=0.4684526849140138, sin(theta)=-0.883488586228963, theta dot=1.7281273577054392\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in range(3):\n",
    "    # Randomly sample an element of this space => Joint Effort between -2.0 and 2.0\n",
    "    an_action = env.action_space.sample()\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"a sample of action = {}\".format(an_action))\n",
    "    \n",
    "    # launch this action to the environnement\n",
    "    observation = env.step(env.action_space.sample())\n",
    "    # observation = (state = [cos(theta), sin(theta), theta dot] , reward, done, info)\n",
    "    \n",
    "    print(\"[{}] observation = {}\".format(i,observation))\n",
    "    print(\"[{}] reward = {}\".format(i,observation[1]))\n",
    "    print(\"[{}] internal state = {}\".format(i,env.state)) # env state is a numpy array [theta, theta_dot]\n",
    "    print(\"[{}] theta={}, cos(theta)={}, sin(theta)={}, theta dot={}\".format(i,env.state[0],np.cos(env.state[0]), np.sin(env.state[0]), env.state[1]))\n",
    "    \n",
    "    #env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Why DDPG (Deep Deterministic Policy Gradient) ?</h1>\n",
    "<br>\n",
    "\n",
    "Deep Q-Learning can deal well with high-dimensional state space (images as an input) \n",
    "but still it cannot deal with high dimensional action spaces (continuous action).\n",
    "\n",
    "DDPG is for dealing with continuous, hence high-dimensional, action spaces.\n",
    "\n",
    "- DDPG used an actor-critic architecture than combine Q-network and policy network.\n",
    "- DDPG is an off-policy algorithm.\n",
    "- DDPG can only be used for environments with continuous action spaces.\n",
    "- DDPG can be thought of as being deep Q-learning for continuous action spaces.\n",
    "\n",
    "<h3>The policy structure is known as the actor</h3>, because it is used to select actions. \n",
    "<h3>The estimated value function is known as the critic</h3>, because it criticizes \n",
    "the actions made by the actor.\n",
    "\n",
    "The policy is basically the agent behavior, a mapping from state to action \n",
    "(in case of deterministic policy) or a distribution of actions (in case of stochastic policy).\n",
    "The output of the policy network is a value that corresponds to the action to be taken on the environment.\n",
    "\n",
    "The critic is a state-value function. After each action selection, the critic evaluates the new state to determine whether things have gone better or worse than expected\n",
    "\n",
    "\n",
    "Actor takes state as input to give action as output, while critic takes both state and action as input to give as output the value of the Q function. \n",
    "<br>\n",
    "The critic uses gradient temporal-difference learning while the actor parameters are learned following policy gradient theorem. \n",
    "<br>\n",
    "The main idea behind this architecture is that the policy network acts producing an action and the Q-network criticize that action.\n",
    "<br>\n",
    "https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "https://github.com/openai/spinningup.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.paperspace.com/physics-control-tasks-with-deep-reinforcement-learning/\n",
    "# https://github.com/antocapp/paperspace-ddpg-tutorial/blob/master/ddpg-pendulum-250.ipynb\n",
    "\n",
    "# When we deal with high-dimensional state space or action spaces \n",
    "# we have to introduce complex and non-linear function approximators \n",
    "# such as deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy or off-policy ?\n",
    "\n",
    "What actually policy (denoted by π) means ?\n",
    "Policy specifies an action a, that is taken in a state s (or more precisely, π is a probability, that an action a is taken in a state s).\n",
    "\n",
    "What types of learning do we have?\n",
    "1. Evaluate Q(s,a) function: predict sum of future discounted rewards, where a is an action and s is a state.\n",
    "2. Find π (actually, π(a|s)), that yields to a maximum reward.\n",
    "\n",
    "On-policy and off-policy learning is only related to the first task: evaluating Q(s,a).\n",
    "\n",
    "The difference is this:<br>\n",
    "- In on-policy learning, the Q(s,a) function is learned from actions that we took using our current policy π(a|s).\n",
    "<br>The update function for the on-policy SARSA algorithm: Q(s,a)←Q(s,a)+α(r+γQ(s′,a′)−Q(s,a)), \n",
    "where a′ is the action, that was taken according to policy π.\n",
    "<br>\n",
    "- In off-policy learning, the Q(s,a) function is learned from taking different actions (for example, random actions). \n",
    "We even don't need a policy at all !\n",
    "<br>The update function for the off-policy Q-learning algorithm: Q(s,a)←Q(s,a)+α(r+γmaxa′Q(s′,a′)−Q(s,a)), \n",
    "where a′ are all actions, that were probed in state s′."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used = cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device used = {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<PendulumEnv<Pendulum-v0>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = True\n",
    "random_seed = 9527\n",
    "\n",
    "if seed:\n",
    "    env.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dimension = 3\n",
      "action dimension = 1\n",
      "max action value = 2.0\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "print(\"state dimension = {}\".format(state_dim))\n",
    "\n",
    "action_dim = env.action_space.shape[0]\n",
    "print(\"action dimension = {}\".format(action_dim))\n",
    "\n",
    "max_action = float(env.action_space.high[0])\n",
    "print(\"max action value = {}\".format(max_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_Val = 1.0000000116860974e-07\n"
     ]
    }
   ],
   "source": [
    "# min value set to tensor(1.0000e-07, device='cuda:0') if device is 'cuda'\n",
    "# or tensor(1.0000e-07) if device is 'cpu'\n",
    "min_Val = torch.tensor(1e-7).float().to(device) \n",
    "print(\"min_Val = {}\".format(min_Val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    '''\n",
    "    Code based on:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "    Expects tuples of (state, next_state, action, reward, done)\n",
    "    '''\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    # data = (state, next_state, action, reward, np.float(done))\n",
    "    # state is a numpy array with 3 float values : cos(theta), sin(theta), theta_dot\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False)) # state  x = [ np.array([v1,v2,v3]), ..., np.array([v1,v2,v3)]) ]\n",
    "            y.append(np.array(Y, copy=False)) # next_state\n",
    "            u.append(np.array(U, copy=False)) # action\n",
    "            r.append(np.array(R, copy=False)) # reward  r = [ np.array(v1), .... , np.array(v1) ]\n",
    "            d.append(np.array(D, copy=False)) # done = [ np.array(v1), .... , np.array(v1) ]\n",
    "\n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "# without reshape(-1, 1 ) :\n",
    "# r = [-11.5446264   ...  -1.30482835 ... -1.01760303]\n",
    "# d = [0. ...  0. ... 0.]\n",
    "\n",
    "# with reshape(-1, 1) (row dimension is unknown, column dimension is 1) :\n",
    "# r = [[-11.5446264] ... [-1.30482835] ... [-1.01760303]]\n",
    "# d = [[0.] ... [0.] ... [O.]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = [ 0.29314158 -0.95606904 -0.78333894]\n",
      "random state array = [[ 0.45193868 -0.892049    2.08262931]\n",
      " [-0.06649502  0.99778676  3.46687659]\n",
      " [-0.59067748  0.80690775 -4.75601588]\n",
      " [-0.99888653 -0.04717731  6.24698897]\n",
      " [ 0.10659679  0.99430233  2.80490138]\n",
      " [-0.33496515 -0.94223052 -4.73208667]\n",
      " [ 0.09759534 -0.99522618 -3.5491674 ]\n",
      " [ 0.35676804 -0.934193    2.56764162]\n",
      " [-0.86246429  0.50611791  5.50533114]\n",
      " [-0.93744507 -0.34813324  6.16768416]]\n",
      "random next state array = [[ 0.51172929 -0.85914675  1.36517888]\n",
      " [-0.26120418  0.96528357  3.95450708]\n",
      " [-0.40605578  0.91384829 -4.2752868 ]\n",
      " [-0.93744507 -0.34813324  6.16768416]\n",
      " [-0.06649502  0.99778676  3.46687659]\n",
      " [-0.57126849 -0.82076325 -5.32964553]\n",
      " [-0.10477128 -0.99449634 -4.0542971 ]\n",
      " [ 0.45193868 -0.892049    2.08262931]\n",
      " [-0.97385323  0.22717807  6.02997303]\n",
      " [-0.78864325 -0.61485106  6.13236114]]\n",
      "random action array = [[-0.3227578]\n",
      " [-1.7380638]\n",
      " [-0.8296782]\n",
      " [-0.2928122]\n",
      " [-0.5583436]\n",
      " [ 0.7274268]\n",
      " [ 1.6085995]\n",
      " [ 1.4375496]\n",
      " [ 0.9670231]\n",
      " [ 1.5051794]]\n",
      "random reward array = [[ -1.64793167]\n",
      " [ -3.88582794]\n",
      " [ -7.11452019]\n",
      " [-13.47787077]\n",
      " [ -2.93034502]\n",
      " [ -5.89693183]\n",
      " [ -3.43210922]\n",
      " [ -2.11575763]\n",
      " [ -9.84867934]\n",
      " [-11.56817024]]\n",
      "random done array = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "tensor state = tensor([[ 0.4519, -0.8920,  2.0826],\n",
      "        [-0.0665,  0.9978,  3.4669],\n",
      "        [-0.5907,  0.8069, -4.7560],\n",
      "        [-0.9989, -0.0472,  6.2470],\n",
      "        [ 0.1066,  0.9943,  2.8049],\n",
      "        [-0.3350, -0.9422, -4.7321],\n",
      "        [ 0.0976, -0.9952, -3.5492],\n",
      "        [ 0.3568, -0.9342,  2.5676],\n",
      "        [-0.8625,  0.5061,  5.5053],\n",
      "        [-0.9374, -0.3481,  6.1677]], device='cuda:0')\n",
      "tensor_next_state = tensor([[ 0.5117, -0.8591,  1.3652],\n",
      "        [-0.2612,  0.9653,  3.9545],\n",
      "        [-0.4061,  0.9138, -4.2753],\n",
      "        [-0.9374, -0.3481,  6.1677],\n",
      "        [-0.0665,  0.9978,  3.4669],\n",
      "        [-0.5713, -0.8208, -5.3296],\n",
      "        [-0.1048, -0.9945, -4.0543],\n",
      "        [ 0.4519, -0.8920,  2.0826],\n",
      "        [-0.9739,  0.2272,  6.0300],\n",
      "        [-0.7886, -0.6149,  6.1324]], device='cuda:0')\n",
      "tensor action = tensor([[-0.3228],\n",
      "        [-1.7381],\n",
      "        [-0.8297],\n",
      "        [-0.2928],\n",
      "        [-0.5583],\n",
      "        [ 0.7274],\n",
      "        [ 1.6086],\n",
      "        [ 1.4375],\n",
      "        [ 0.9670],\n",
      "        [ 1.5052]], device='cuda:0')\n",
      "tensor reward = tensor([[ -1.6479],\n",
      "        [ -3.8858],\n",
      "        [ -7.1145],\n",
      "        [-13.4779],\n",
      "        [ -2.9303],\n",
      "        [ -5.8969],\n",
      "        [ -3.4321],\n",
      "        [ -2.1158],\n",
      "        [ -9.8487],\n",
      "        [-11.5682]], device='cuda:0')\n",
      "tensor done = tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Tests Replay Buffer\n",
    "\n",
    "capacity = 1000000\n",
    "replay_buffer = Replay_buffer(capacity)\n",
    "\n",
    "state = env.reset()\n",
    "print(\"initial state = {}\".format(state))\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.push((state, next_state, action, reward, np.float(done)))\n",
    "    state = next_state\n",
    "    \n",
    "batch_size = 10\n",
    "state_ar, next_state_ar, action_ar, reward_ar, done_ar = replay_buffer.sample(batch_size)\n",
    "print(\"random state array = {}\".format(state_ar))\n",
    "print(\"random next state array = {}\".format(next_state_ar))\n",
    "print(\"random action array = {}\".format(action_ar))\n",
    "print(\"random reward array = {}\".format(reward_ar))\n",
    "print(\"random done array = {}\".format(done_ar))\n",
    "\n",
    "\n",
    "tensor_state = torch.FloatTensor(state_ar).to(device)\n",
    "tensor_next_state = torch.FloatTensor(next_state_ar).to(device)\n",
    "tensor_action = torch.FloatTensor(action_ar).to(device)\n",
    "tensor_reward = torch.FloatTensor(reward_ar).to(device)\n",
    "tensor_done = torch.FloatTensor(1-done_ar).to(device)\n",
    "\n",
    "print(\"tensor state = {}\".format(tensor_state))\n",
    "print(\"tensor_next_state = {}\".format(tensor_next_state))\n",
    "print(\"tensor action = {}\".format(tensor_action))\n",
    "print(\"tensor reward = {}\".format(tensor_reward))\n",
    "print(\"tensor done = {}\".format(tensor_done))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    # state_dim = 3, action_dim = 1, max_action = 2.0\n",
    "    # state = [cos(theta), sin(theta), theta_dot]\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400) # input is state (3 float values : [cos(theta), sin(theta), theta_dot])\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim) # output is 1 action value\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x)) # with relu, x has positives values\n",
    "        x = F.relu(self.l2(x)) # with relu, x has positives values\n",
    "        # tanh give a value between -1 and 1 (pendulum normal action is between -2.0 and 2.0).\n",
    "        # max_action = 2.0\n",
    "        # so forward action : tanh * max_action\n",
    "        x = self.max_action * torch.tanh(self.l3(x))  \n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # state_dim = 3, action_dim = 1\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400) # In Critic network, input is state plus action\n",
    "        self.l2 = nn.Linear(400 , 300)\n",
    "        self.l3 = nn.Linear(300, 1) # the output is 1 Q Value\n",
    "\n",
    "    # x = state, u = action\n",
    "    def forward(self, x, u):\n",
    "        x = F.relu(self.l1(torch.cat([x, u], 1))) # cat -> x\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x) # x is the Q Value\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = [0.08166688 0.99665968 0.50738284]\n",
      "cuda tensor state = tensor([0.0817, 0.9967, 0.5074], device='cuda:0')\n",
      "cuda tensor state reshape = tensor([[0.0817, 0.9967, 0.5074]], device='cuda:0')\n",
      "action = [-0.05473555]\n",
      "action = tensor([[-0.2503]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "action = tensor([[-0.2503]], grad_fn=<CopyBackwards>)\n",
      "action = tensor([[-0.2503]])\n",
      "action = [[-0.250273]]\n",
      "action = [-0.250273]\n",
      "action = [-0.250273]\n"
     ]
    }
   ],
   "source": [
    "# state_dim = 3, action_dim = 1, max_action = 2.0\n",
    "actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\n",
    "# Get a state ( test with an initial state given with reset() ).\n",
    "# state is a np.array (numpy array) that contains [cos(theta), sin(theta), theta dot].\n",
    "# [-0.12477272 -0.99218535  0.02429846]\n",
    "state = env.reset()\n",
    "print(\"state = {}\".format(state))\n",
    "\n",
    "# Actor network need a tensor as input, so transform state array into a tensor,\n",
    "# and take advantage of using cuda.\n",
    "# tensor([-0.1248, -0.9922,  0.0243], device='cuda:0')\n",
    "tensor_state = torch.FloatTensor(state).to(device)\n",
    "print(\"cuda tensor state = {}\".format(tensor_state))\n",
    "\n",
    "# But this is not enough, in fact, the input of the network need a \"batch\" tensor of 3 values. \n",
    "# Therefore we need to add an extra dimension using the reshape function.\n",
    "# tensor([[-0.1248, -0.9922,  0.0243]], device='cuda:0')\n",
    "tensor_state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "print(\"cuda tensor state reshape = {}\".format(tensor_state))\n",
    "\n",
    "# Send now this \"batch\" tensor into the Actor network, and get the action associated.\n",
    "# actor(tensor_state) is equivalent to actor.forward(tensor_state)\n",
    "action = actor(tensor_state).cpu().data.numpy().flatten()\n",
    "print(\"action = {}\".format(action_flat))\n",
    "\n",
    "# As you can see the previous line is very complicated, \n",
    "# we are going to dissect it step by step.\n",
    "\n",
    "# 1- send the tensor_state like this\n",
    "# this returns a \"batch\" tensor of actions (Only 1 in our case) using cuda.\n",
    "# tensor([[-0.2250]], device='cuda:0', grad_fn=<MulBackward0>)\n",
    "action = actor(tensor_state)\n",
    "print(\"action = {}\".format(action))\n",
    "\n",
    "# 2- let's bring it back to cpu.\n",
    "# tensor([[-0.2250]], grad_fn=<CopyBackwards>)\n",
    "action = action.cpu()\n",
    "print(\"action = {}\".format(action))\n",
    "\n",
    "# 3- detach() detaches the output from the computationnal graph. \n",
    "# So no gradient will be backproped along this variable.\n",
    "# old version of pytorch used : action = action.data\n",
    "# tensor([[-0.2503]])\n",
    "action = action.detach()\n",
    "print(\"action = {}\".format(action))\n",
    "\n",
    "\n",
    "action = action.numpy()\n",
    "print(\"action = {}\".format(action))\n",
    "action = action.flatten()\n",
    "print(\"action = {}\".format(action))\n",
    "action = np.float32(action)\n",
    "print(\"action = {}\".format(action))\n",
    "\n",
    "#state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "#        return self.actor(state).cpu().data.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
