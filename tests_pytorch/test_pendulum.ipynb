{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendulum-v0\n",
    "https://github.com/openai/gym/wiki/Pendulum-v0\n",
    "    \n",
    "The problem  is to  keep a frictionless pendulum standing up.\n",
    "\n",
    "Action :\n",
    "Box(1) -> Joint Effort between -2.0 and 2.0\n",
    "\n",
    "Starting state :\n",
    "Random angle from −π to π, and random velocity between -1 and 1\n",
    "\n",
    "Reward :\n",
    "Reward = -(theta^2 + 0.1\\*theta_dt^2 + 0.001\\*action^2)\n",
    "\n",
    "Reward is based on the angle of the pendulum (1), the angular velocity (2) of the pendulum, \n",
    "and the force applied (3).\n",
    "Agents get increased reward for keeping the pendulum (1) upright, (2) still, and (3) using little force.\n",
    "\n",
    "Theta is normalized between -pi and pi\n",
    "\n",
    "Therefore, the lowest reward is -(pi^2 + 0.1\\*8^2 + 0.001\\*2^2) = -16.2736044, \n",
    "and the highest reward is 0. \n",
    "In essence, the goal is to remain at zero angle (vertical), \n",
    "with the least rotational velocity, and the least effort.\n",
    "\n",
    "\n",
    "Episode Termination :\n",
    "There is no specified termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02603071, -0.99966114,  0.14493233])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Box(-2.0, 2.0, (1,), float32)\n",
      "observation space = Box(-8.0, 8.0, (3,), float32)\n",
      "observation = [-0.97323528 -0.22981098 -0.91775318]\n",
      "internal state = [-2.9097092  -0.91775318]\n",
      "theta=-2.9097091957090253, cos(theta)=-0.9732352821977132, sin(theta)=-0.22981097773069434, theta dot=-0.9177531794044946\n"
     ]
    }
   ],
   "source": [
    "print(\"action space = {}\".format(env.action_space))\n",
    "# Box(-2.0, 2.0, (1,), float32)\n",
    "# 1 float value between -2.0 and 2.0\n",
    "# Action = between -2.0 and 2.0\n",
    "\n",
    "print(\"observation space = {}\".format(env.observation_space))\n",
    "# Box(-8.0, 8.0, (3,), float32)\n",
    "# 3 float values between -8.0 and 8.0\n",
    "# Observation/State = [cos(theta), sin(theta), theta dot]\n",
    "# index 0 -> cos(theta) [-1.0, 1.0] \n",
    "# index 1 -> sin(theta) [-1.0, 1.0]\n",
    "# index 2 -> theta dot [-8.0, 8.0]\n",
    "# The angles(thetas) are passed through the sin() and cos() function so that the observations are in the range [-1,1]. \n",
    "# This fixed range of [-1,1] helps in stabilising the training in the neural networks\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"observation = {}\".format(observation))\n",
    "print(\"internal state = {}\".format(env.state)) # env state is a numpy array [theta, theta_dot]\n",
    "print(\"theta={}, cos(theta)={}, sin(theta)={}, theta dot={}\".format(env.state[0],np.cos(env.state[0]), np.sin(env.state[0]), env.state[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "a sample of action = [-1.0639498]\n",
      "[0] observation = (array([-0.99326451, -0.11586893,  0.56330775]), -9.361322651388539, False, {})\n",
      "[0] reward = -9.361322651388539\n",
      "[0] internal state = [-3.02546288  0.56330775]\n",
      "[0] theta=-3.0254628764432385, cos(theta)=-0.9932645121729079, sin(theta)=-0.11586892964861335, theta dot=0.5633077549624914\n",
      "------------------------------------------------\n",
      "a sample of action = [0.7401112]\n",
      "[1] observation = (array([-0.9898852 , -0.14187071,  0.52442409]), -9.18525965634952, False, {})\n",
      "[1] reward = -9.18525965634952\n",
      "[1] internal state = [-2.99924167  0.52442409]\n",
      "[1] theta=-2.9992416721648345, cos(theta)=-0.9898851967274155, sin(theta)=-0.1418707062783789, theta dot=0.5244240855680763\n",
      "------------------------------------------------\n",
      "a sample of action = [0.2110196]\n",
      "[2] observation = (array([-0.98758351, -0.1570949 ,  0.30794708]), -9.023491171561112, False, {})\n",
      "[2] reward = -9.023491171561112\n",
      "[2] internal state = [-2.98384432  0.30794708]\n",
      "[2] theta=-2.9838443182573107, cos(theta)=-0.9875835116321918, sin(theta)=-0.15709489982882435, theta dot=0.3079470781504757\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in range(3):\n",
    "    # Randomly sample an element of this space => Joint Effort between -2.0 and 2.0\n",
    "    an_action = env.action_space.sample()\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"a sample of action = {}\".format(an_action))\n",
    "    \n",
    "    # launch this action to the environnement\n",
    "    observation = env.step(env.action_space.sample())\n",
    "    # observation = (state = [cos(theta), sin(theta), theta dot] , reward, done, info)\n",
    "    \n",
    "    print(\"[{}] observation = {}\".format(i,observation))\n",
    "    print(\"[{}] reward = {}\".format(i,observation[1]))\n",
    "    print(\"[{}] internal state = {}\".format(i,env.state)) # env state is a numpy array [theta, theta_dot]\n",
    "    print(\"[{}] theta={}, cos(theta)={}, sin(theta)={}, theta dot={}\".format(i,env.state[0],np.cos(env.state[0]), np.sin(env.state[0]), env.state[1]))\n",
    "    \n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we deal with high-dimensional state space or action spaces \n",
    "# we have to introduce complex and non-linear function approximators \n",
    "# such as deep neural networks\n",
    "\n",
    "# DDPG is for dealing with continuous, hence high-dimensional, action spaces in a Reinforcement Learning framework.\n",
    "# DDPG = Deep Deterministic Policy Gradient\n",
    "# https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "# https://github.com/openai/spinningup.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.paperspace.com/physics-control-tasks-with-deep-reinforcement-learning/\n",
    "# https://github.com/antocapp/paperspace-ddpg-tutorial/blob/master/ddpg-pendulum-250.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
