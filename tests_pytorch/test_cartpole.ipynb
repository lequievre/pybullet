{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cartpole.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "    \n",
    "# Description:\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to\n",
    "prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "# Source:\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
    "# Observation:\n",
    "    Type: Box(4)\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "# Actions:\n",
    "    Type: Discrete(2)\n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right\n",
    "    Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "# Reward:\n",
    "    Reward is 1 for every step taken, including the termination step\n",
    "# Starting State:\n",
    "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "# Episode Termination:\n",
    "    - Pole Angle is more than 12 degrees.\n",
    "    - Cart Position is more than 2.4 (center of the cart reaches the edge of the display).\n",
    "    - Episode length is greater than 200.\n",
    "    Solved Requirements:\n",
    "    Considered solved when the average return is greater than or equal to\n",
    "    195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')  # CartPole-v0 only runs for 200 steps. CartPole-v1 runs for 500 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gym_openai.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment comes with an action_space and an observation_space :\n",
    "\n",
    "These attributes are of type Space, and they describe the format of valid actions and observations.\n",
    "\n",
    "The 'Discrete' space allows a fixed range of non-negative numbers.\n",
    "- Discrete(2) -> in this case valid actions are either 0 or 1.\n",
    "- Discrete(8) -> Set with 8 elements {0, 1, 2, ..., 7}\n",
    "\n",
    "The 'Box' space represents an n-dimensional box.\n",
    "- Box(4,) -> valid observations will be an array of 4 numbers (in that example, there is only one dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(2)\n",
      "nb actions = 2\n",
      "a sample of action = 0\n",
      "a sample of action = 0\n",
      "a sample of action = 1\n",
      "observation space = Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "observation space shape = (4,)\n",
      "observation high = [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "observation low = [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "x cart pos high = 4.800000190734863\n",
      "x cart pos low = -4.800000190734863\n",
      "x cart velocity high = 3.4028234663852886e+38\n",
      "x cart velocity low = -3.4028234663852886e+38\n",
      "Angular pole pos high = 0.41887903213500977\n",
      "Angular pole pos low = -0.41887903213500977\n",
      "Angular pole velocity high = 3.4028234663852886e+38\n",
      "Angular pole velocity low = -3.4028234663852886e+38\n"
     ]
    }
   ],
   "source": [
    "# About action and observation space\n",
    "\n",
    "# action_space\n",
    "print(\"action space = {}\".format(env.action_space)) # Discrete(2)\n",
    "print(\"nb actions = {}\".format(env.action_space.n)) # 2\n",
    "# 0 Push cart to the left\n",
    "# 1 Push cart to the right\n",
    "\n",
    "for _ in range(3):\n",
    "    # Randomly sample an element of this space => random(2) (just to test)\n",
    "    print(\"a sample of action = {}\".format(env.action_space.sample()))\n",
    "    \n",
    "# observation_space\n",
    "# observation = x position of cart, x velocity of cart, angular position of pole, angular velocity of pole\n",
    "print(\"observation space = {}\".format(env.observation_space))\n",
    "print(\"observation space shape = {}\".format(env.observation_space.shape)) # (4,)\n",
    "\n",
    "# We can also check the observation Box’s bounds\n",
    "print(\"observation high = {}\".format(env.observation_space.high))\n",
    "print(\"observation low = {}\".format(env.observation_space.low))\n",
    "\n",
    "# Observation limits in details\n",
    "print(\"x cart pos high = {}\".format(env.observation_space.high[0])) # 4.8\n",
    "print(\"x cart pos low = {}\".format(env.observation_space.low[0])) # -4.8\n",
    "\n",
    "print(\"x cart velocity high = {}\".format(env.observation_space.high[1])) # Inf\n",
    "print(\"x cart velocity low = {}\".format(env.observation_space.low[1])) # -Inf\n",
    "\n",
    "print(\"Angular pole pos high = {}\".format(env.observation_space.high[2])) # 0.418 rad (~ 24 deg)\n",
    "print(\"Angular pole pos low = {}\".format(env.observation_space.low[2])) # -0.418 rad (~ -24 deg)\n",
    "\n",
    "print(\"Angular pole velocity high = {}\".format(env.observation_space.high[3])) # Inf\n",
    "print(\"Angular pole velocity low = {}\".format(env.observation_space.low[3])) # -Inf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'step' function returns four values :\n",
    "\n",
    "- observation (object): \n",
    "an environment-specific object representing your observation of the environment. \n",
    "For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "- reward (float): \n",
    "amount of reward achieved by the previous action. \n",
    "The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "- done (boolean): \n",
    "whether it’s time to reset the environment again. \n",
    "Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. \n",
    "(For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "- info (dict): \n",
    "diagnostic information useful for debugging. \n",
    "It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). \n",
    "However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "\n",
    "Example for CartPole : array([-0.00842369, -0.17244261, -0.0034994 ,  0.24360119]), 1.0, False, {}\n",
    "    \n",
    "    observation = array([x position of cart, x velocity of cart, angular position of pole, angular velocity of pole])\n",
    "    reward = 1.0\n",
    "    done = False\n",
    "    info = {}\n",
    "    \n",
    "# The 'reset' function returns :\n",
    "\n",
    "- observation (object): \n",
    "an environment-specific object representing your observation of the environment. \n",
    "For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "\n",
    "# About 'done' :\n",
    "\n",
    "- The environment will return done=True if either 200 timesteps have elapsed (episode success) \n",
    "or if the pole has fallen over (angular position of the pole has reached +- 12 degrees) \n",
    "or the cart has left the simulation space (cart position has reached +- 2.4), in which case the episode failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation = [-0.01988554  0.02129117  0.03425103 -0.01337742]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode : 20.95\n"
     ]
    }
   ],
   "source": [
    "# Example 1 :\n",
    "# Selecting actions randomly\n",
    "# Calulating reward per episode\n",
    "\n",
    "from torch import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Just to test initial observation\n",
    "initial_obs = env.reset() # reset environment\n",
    "print(\"initial observation = {}\".format(initial_obs))\n",
    "\n",
    "reward_array = []\n",
    "episode_count = 100\n",
    "\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('sum reward')\n",
    "\n",
    "for i in range(episode_count):\n",
    "    # obs = reset initial obs, done = False, sum_reward per episode = 0\n",
    "    obs, done, sum_reward = env.reset(), False, 0\n",
    "    \n",
    "    while (done != True) :\n",
    "        # Tensor A size : torch.Size([1])\n",
    "        A =  randint(0,env.action_space.n,(1,))  # get a random action from 0 (inclusive) to 2 (exclusive)\n",
    "        \n",
    "        #env.render() # will open a GUI window and show you the cartpole.\n",
    "        \n",
    "        obs, reward, done, info = env.step(A.item()) # apply the random action to the environment\n",
    "        sum_reward += reward\n",
    "        \n",
    "    reward_array.append(sum_reward) # add sum_reward to reward array\n",
    "    \n",
    "env.close() # close environment\n",
    "\n",
    "t = np.arange(0, episode_count, 1)\n",
    "plt.plot(t,reward_array,'ro') # plot with pyplot function the reward array\n",
    "plt.show()\n",
    "\n",
    "print(\"Average reward per episode :\",sum(reward_array)/ len(reward_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Solution that take a random action is very bad.\n",
    "# In that case, there is 100 episodes and each episode has a 21 as cumulated reward.\n",
    "# (Reward is 1 for every step taken, including the termination step)\n",
    "# The environment will return done=True if either 200 timesteps have elapsed (episode success)\n",
    "# or if the pole has fallen over (angular position of the pole has reached +- 12 degrees) \n",
    "# or the cart has left the simulation space (cart position has reached +- 2.4), in which case the episode failed.\n",
    "\n",
    "# The objective now is to define a policy which allows us to receive 200 rewards per episode.\n",
    "# By using reinforcement learning techniques such as the DQN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
