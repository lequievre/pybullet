{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cartpole.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "    \n",
    "# Description:\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to\n",
    "prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "# Source:\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
    "# Observation:\n",
    "    Type: Box(4)\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "# Actions:\n",
    "    Type: Discrete(2)\n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right\n",
    "    Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "# Reward:\n",
    "    Reward is 1 for every step taken, including the termination step\n",
    "# Starting State:\n",
    "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "# Episode Termination:\n",
    "    - Pole Angle is more than 12 degrees.\n",
    "    - Cart Position is more than 2.4 (center of the cart reaches the edge of the display).\n",
    "    - Episode length is greater than 200.\n",
    "    Solved Requirements:\n",
    "    Considered solved when the average return is greater than or equal to\n",
    "    195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "\n",
    "# Agent are a single entity (the algorithm you are writing) that, through trial and error, \n",
    "# learn how to exploit their actions to gain reward.\n",
    "\n",
    "# It sends actions to the environment, and the environment replies with observations (states)\n",
    "# and rewards (that is, a score).\n",
    "\n",
    "# The agent must have a policy/strategy (to select an action) and have a mechanism \n",
    "# to improve this strategy as it interacts more and more with the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')  # CartPole-v0 only runs for 200 steps. CartPole-v1 runs for 500 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gym_openai.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment comes with an action_space and an observation_space :\n",
    "\n",
    "These attributes are of type Space, and they describe the format of valid actions and observations.\n",
    "\n",
    "The 'Discrete' space allows a fixed range of non-negative numbers.\n",
    "- Discrete(2) -> in this case valid actions are either 0 or 1.\n",
    "- Discrete(8) -> Set with 8 elements {0, 1, 2, ..., 7}\n",
    "\n",
    "The 'Box' space represents an n-dimensional box.\n",
    "- Box(4,) -> valid observations will be an array of 4 numbers (in that example, there is only one dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(2)\n",
      "nb actions = 2\n",
      "a sample of action = 1\n",
      "a sample of action = 0\n",
      "a sample of action = 1\n",
      "observation space = Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "observation space shape = (4,)\n",
      "observation high = [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "observation low = [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "x cart pos high = 4.800000190734863\n",
      "x cart pos low = -4.800000190734863\n",
      "x cart velocity high = 3.4028234663852886e+38\n",
      "x cart velocity low = -3.4028234663852886e+38\n",
      "Angular pole pos high = 0.41887903213500977\n",
      "Angular pole pos low = -0.41887903213500977\n",
      "Angular pole velocity high = 3.4028234663852886e+38\n",
      "Angular pole velocity low = -3.4028234663852886e+38\n"
     ]
    }
   ],
   "source": [
    "# About action and observation space\n",
    "\n",
    "# action_space\n",
    "print(\"action space = {}\".format(env.action_space)) # Discrete(2)\n",
    "print(\"nb actions = {}\".format(env.action_space.n)) # 2\n",
    "# 0 Push cart to the left\n",
    "# 1 Push cart to the right\n",
    "\n",
    "for _ in range(3):\n",
    "    # Randomly sample an element of this space => random(2) (just to test)\n",
    "    print(\"a sample of action = {}\".format(env.action_space.sample()))\n",
    "    \n",
    "# observation_space\n",
    "# observation = x position of cart, x velocity of cart, angular position of pole, angular velocity of pole\n",
    "print(\"observation space = {}\".format(env.observation_space))\n",
    "print(\"observation space shape = {}\".format(env.observation_space.shape)) # (4,)\n",
    "\n",
    "# We can also check the observation Box’s bounds\n",
    "print(\"observation high = {}\".format(env.observation_space.high))\n",
    "print(\"observation low = {}\".format(env.observation_space.low))\n",
    "\n",
    "# Observation limits in details\n",
    "print(\"x cart pos high = {}\".format(env.observation_space.high[0])) # 4.8\n",
    "print(\"x cart pos low = {}\".format(env.observation_space.low[0])) # -4.8\n",
    "\n",
    "print(\"x cart velocity high = {}\".format(env.observation_space.high[1])) # Inf\n",
    "print(\"x cart velocity low = {}\".format(env.observation_space.low[1])) # -Inf\n",
    "\n",
    "print(\"Angular pole pos high = {}\".format(env.observation_space.high[2])) # 0.418 rad (~ 24 deg)\n",
    "print(\"Angular pole pos low = {}\".format(env.observation_space.low[2])) # -0.418 rad (~ -24 deg)\n",
    "\n",
    "print(\"Angular pole velocity high = {}\".format(env.observation_space.high[3])) # Inf\n",
    "print(\"Angular pole velocity low = {}\".format(env.observation_space.low[3])) # -Inf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'step' function returns four values :\n",
    "\n",
    "- observation (object): \n",
    "an environment-specific object representing your observation of the environment. \n",
    "For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "- reward (float): \n",
    "amount of reward achieved by the previous action. \n",
    "The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "- done (boolean): \n",
    "whether it’s time to reset the environment again. \n",
    "Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. \n",
    "(For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "- info (dict): \n",
    "diagnostic information useful for debugging. \n",
    "It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). \n",
    "However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "\n",
    "Example for CartPole : array([-0.00842369, -0.17244261, -0.0034994 ,  0.24360119]), 1.0, False, {}\n",
    "    \n",
    "    observation = array([x position of cart, x velocity of cart, angular position of pole, angular velocity of pole])\n",
    "    reward = 1.0\n",
    "    done = False\n",
    "    info = {}\n",
    "    \n",
    "# The 'reset' function returns :\n",
    "\n",
    "- observation (object): \n",
    "an environment-specific object representing your observation of the environment. \n",
    "For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "\n",
    "# About 'done' :\n",
    "\n",
    "- The environment will return done=True if either 200 timesteps have elapsed (episode success) \n",
    "or if the pole has fallen over (angular position of the pole has reached +- 12 degrees) \n",
    "or the cart has left the simulation space (cart position has reached +- 2.4), in which case the episode failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation = [-0.03107349  0.04734051  0.01177347 -0.00649577]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode : 21.07\n"
     ]
    }
   ],
   "source": [
    "# Example 1 :\n",
    "# Selecting actions randomly\n",
    "# Calulating reward per episode\n",
    "\n",
    "from torch import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Just to test initial observation\n",
    "initial_obs = env.reset() # reset environment\n",
    "print(\"initial observation = {}\".format(initial_obs))\n",
    "\n",
    "reward_array = []\n",
    "episode_count = 100\n",
    "\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('sum reward')\n",
    "\n",
    "for i in range(episode_count):\n",
    "    # obs = reset initial obs, done = False, sum_reward per episode = 0\n",
    "    obs, done, sum_reward = env.reset(), False, 0\n",
    "    \n",
    "    while (done != True) :\n",
    "        # Tensor A size : torch.Size([1])\n",
    "        A =  randint(0,env.action_space.n,(1,))  # get a random action from 0 (inclusive) to 2 (exclusive)\n",
    "        \n",
    "        #env.render() # will open a GUI window and show you the cartpole.\n",
    "        \n",
    "        obs, reward, done, info = env.step(A.item()) # apply the random action to the environment\n",
    "        sum_reward += reward\n",
    "        \n",
    "    reward_array.append(sum_reward) # add sum_reward to reward array\n",
    "    \n",
    "env.close() # close environment\n",
    "\n",
    "t = np.arange(0, episode_count, 1)\n",
    "plt.plot(t,reward_array,'ro') # plot with pyplot function the reward array\n",
    "plt.show()\n",
    "\n",
    "print(\"Average reward per episode :\",sum(reward_array)/ len(reward_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Solution that take a random action is very bad.\n",
    "# In that case, there is 100 episodes and each episode has a 21 as cumulated reward.\n",
    "# (Reward is 1 for every step taken, including the termination step)\n",
    "# The environment will return done=True if either 200 timesteps have elapsed (episode success)\n",
    "# or if the pole has fallen over (angular position of the pole has reached +- 12 degrees) \n",
    "# or the cart has left the simulation space (cart position has reached +- 2.4), in which case the episode failed.\n",
    "\n",
    "# The objective now is to define a policy which allows us to receive 200 rewards per episode.\n",
    "# By using reinforcement learning techniques such as the DQN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN algorithm represent a policy via a Deep Neural Network (DNN).\n",
    "\n",
    "# This DNN will look at the given observation and will tell us which action is best to take in the current state.\n",
    "# A kind of function approximator.\n",
    "# In fact, the network takes a state(‘s) as the input and produces the Q-values for every action in the action space.\n",
    "# Then we just have to take the max Q-value to take the corresponding action in the environment.\n",
    "# best_action = arg max(DNN predicted Q-values ).\n",
    "\n",
    "# We can evaluate and correct the DNN by using any loss function (Mean Squared Error) \n",
    "# between predicted and target returns.\n",
    "\n",
    "# Since we don’t know the “target” as it’s a reinforcement learning \n",
    "# (not supervised so no labels) so we have to estimate it : target = R + γ max A` Q(S`, A`)\n",
    "# We get the reward R and Next state S` when we perform an action in the environment.\n",
    "# R → the current immediate reward\n",
    "# S` → Next state\n",
    "# max A` Q(S`, A`) → max(\"target DNN\" output list of Q-values for the next state)\n",
    "# γ → the discount factor γ → {0,1}\n",
    "# To calculate Q(S`, A`)  we maintain another network we call the \"target network\", \n",
    "# and we assign the \"actual network\" weights to the \"target network\" for every N iterations.\n",
    "# The reason for this is we update the actual network gradients for every frame/action \n",
    "# so the network keeps on changing so it’s not feasible to use the same network for \n",
    "# calculating the actual reward.\n",
    "\n",
    "# Later, with this loss information, we can use Gradient Descent to optimize \n",
    "# the weights of the DNN to minimize this loss. \n",
    "# (update it’s gradients using backprop to converge)\n",
    "\n",
    "# Training such a network requires a lot of data, but even then, it is not guaranteed \n",
    "# to converge on the optimal value function. \n",
    "# In fact, there are situations where the network weights can oscillate or diverge, \n",
    "# due to the high correlation between actions and states.\n",
    "# In order to solve this, we need two techniques : Experience Replay and Target Network\n",
    "\n",
    "# Experience Replay :\n",
    "# -----------------\n",
    "# We can prevent action values from oscillating or diverging catastrophically using\n",
    "# a large buffer of our past experience and sample training data from it, \n",
    "# instead of using our latest experience. \n",
    "# This technique is called replay buffer or experience buffer. \n",
    "# The replay buffer contains a collection of experience tuples (S, A, R, S′).\n",
    "# The act of sampling a small batch of tuples from the replay buffer in order \n",
    "# to learn is known as experience replay.\n",
    "# (using a random subset of these experiences to update the Q-network, \n",
    "# rather than using just the single most recent experience, \n",
    "# which also allows us to break the correlation between subsequent steps in the environment)\n",
    "\n",
    "# Target Network :\n",
    "# --------------\n",
    "# The Bellman equation provides us with the value of Q(s, a) via Q(s’, a’). (s' = next state) \n",
    "# However, both the states s and s’ have only one step between them.\n",
    "# When we perform an update of our Neural Networks’ parameters to make Q(s, a) closer \n",
    "# to the desired result, we can indirectly alter the value produced for Q(s’, a’) \n",
    "# and other states nearby. This can make our training very unstable.\n",
    "# To make training more stable, we use a second Q-network called \"target network\", \n",
    "# by which we keep a copy of our neural network and use it for the Q(s’, a’) value \n",
    "# in the Bellman equation.\n",
    "# The predicted Q values of this second Q-network called the target network, \n",
    "# are used to backpropagate through and train the main Q-network. \n",
    "# The target network’s parameters are not trained, but they are periodically synchronized \n",
    "# with the parameters of the main Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference :\n",
    "# https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
    "# https://github.com/mahakal001/reinforcement-learning\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    # Constructor of the class\n",
    "    def __init__(self, seed, layer_sizes, lr, sync_freq, exp_replay_size):\n",
    "        torch.manual_seed(seed) # set torch seed value\n",
    "        self.q_net = self.build_nn(layer_sizes) # create Q main network\n",
    "        self.target_net = copy.deepcopy(self.q_net) # create Q target network with a copy of Q main.\n",
    "        self.q_net.cuda() # use cuda device\n",
    "        self.target_net.cuda() # use cuda device\n",
    "        self.loss_fn = torch.nn.MSELoss() # use loss function : mean squared error (squared L2 norm)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr) # use Adam optimizer on the Q main net parameters.\n",
    "\n",
    "        self.network_sync_freq = sync_freq # set frequency of synchro between Q main and Q target net.\n",
    "        self.network_sync_counter = 0\n",
    "        self.gamma = torch.tensor(0.95).float().cuda() # set discount factor value for the futur rewards.\n",
    "        self.experience_replay = deque(maxlen=exp_replay_size) # create a replay buffer.\n",
    "        return\n",
    "    \n",
    "    # Build the Q main net :\n",
    "    # Sequential(\n",
    "    #  (0): Linear(in_features=4, out_features=64, bias=True)\n",
    "    #  (1): Tanh()\n",
    "    #  (2): Linear(in_features=64, out_features=2, bias=True)\n",
    "    #  (3): Identity()\n",
    "    # )\n",
    "    def build_nn(self, layer_sizes):\n",
    "        assert len(layer_sizes) > 1\n",
    "        layers = []\n",
    "        for index in range(len(layer_sizes) - 1):\n",
    "            linear = nn.Linear(layer_sizes[index], layer_sizes[index + 1])\n",
    "            act = nn.Tanh() if index < len(layer_sizes) - 2 else nn.Identity()\n",
    "            layers += (linear, act)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # Load q main net parameters from a file named \"cartpole-dqn.pth\n",
    "    def load_pretrained_model(self, model_path):\n",
    "        self.q_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Save parameters of q main net to a file named \"cartpole-dqn.pth\"\n",
    "    def save_trained_model(self, model_path=\"cartpole-dqn.pth\"):\n",
    "        torch.save(self.q_net.state_dict(), model_path)\n",
    "\n",
    "    # From a state, give me an action (0 or 1)\n",
    "    def get_action(self, state, action_space_len, epsilon):\n",
    "        # We do not require gradient at this point, because this function will be used either\n",
    "        # during experience collection or during inference \n",
    "        with torch.no_grad():\n",
    "            Qp = self.q_net(torch.from_numpy(state).float().cuda()) # forward state in q main net and get Q values (2 values).\n",
    "        \n",
    "        # Q = max value of 2 Q Values (Qp), A = index of the max (0 or 1)\n",
    "        Q, A = torch.max(Qp, axis=0) \n",
    "        \n",
    "        # We can either choose the action corresponding to maximum Q-value(exploitation) \n",
    "        # or with a small probability, epsilon, a random action can be selected(exploration)\n",
    "        A = A if torch.rand(1, ).item() > epsilon else torch.randint(0, action_space_len, (1,))\n",
    "        \n",
    "        # A is a Tensor with 1 value (0 or 1) = Action\n",
    "        return A\n",
    "\n",
    "    # get Q value (the max) of next state by using Q target net.\n",
    "    def get_q_next(self, state):\n",
    "        with torch.no_grad():\n",
    "            qp = self.target_net(state)\n",
    "        q, _ = torch.max(qp, axis=1)\n",
    "        return q\n",
    "\n",
    "    # Add a new experience into replay buffer\n",
    "    # Example : send 1 as step to the env (Push cart to the right)\n",
    "    # obs = env.reset()\n",
    "    # obs_next, reward, done, _ = env.step(1)\n",
    "    # experience = [obs, 1, reward, obs_next]\n",
    "    # -> experience is a list of 4 elements [obs, action, reward, next obs]\n",
    "    def collect_experience(self, experience):\n",
    "        self.experience_replay.append(experience)\n",
    "        return\n",
    "\n",
    "    # Extract a sample of size sample_size from replay buffer\n",
    "    # For example : \n",
    "    # sample_from_experience(2) give :\n",
    "    # ( tensor([[ 0.0382,  0.0329, -0.0285, -0.0199],[-0.0285, -0.0212, -0.0015, -0.0407]]),\n",
    "    # tensor([0., 0.]), tensor([1., 1.]),\n",
    "    # tensor([[ 0.0389, -0.1618, -0.0289,  0.2637],[-0.0289, -0.2163, -0.0023,  0.2515]]) )\n",
    "    # A tuple composed with a tensor of 2 obs/states, a tensor of 2 actions,\n",
    "    # a tensor of 2 rewards, a tensor of 2 next obs/states\n",
    "    def sample_from_experience(self, sample_size):\n",
    "        if len(self.experience_replay) < sample_size:\n",
    "            sample_size = len(self.experience_replay)\n",
    "            \n",
    "        # returns a random sample_size length list of items (experiences) chosen from the replay buffer\n",
    "        sample = random.sample(self.experience_replay, sample_size)\n",
    "        \n",
    "        # Each exp is a list -> [obs, action, reward, next obs]\n",
    "        s = torch.tensor([exp[0] for exp in sample]).float() # create a tensor of states\n",
    "        a = torch.tensor([exp[1] for exp in sample]).float() # create a tensor of actions\n",
    "        rn = torch.tensor([exp[2] for exp in sample]).float() # create a tensor of rewards\n",
    "        sn = torch.tensor([exp[3] for exp in sample]).float() # create a tensor of next states\n",
    "        return s, a, rn, sn\n",
    "    \n",
    "    # train Q main network with a batch of sample replay buffer\n",
    "    def train(self, batch_size):\n",
    "        s, a, rn, sn = self.sample_from_experience(sample_size=batch_size)\n",
    "        if self.network_sync_counter == self.network_sync_freq:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            self.network_sync_counter = 0\n",
    "\n",
    "        # predict expected return of current state using main network\n",
    "        qp = self.q_net(s.cuda()) # s is a Tensor \n",
    "        pred_return, _ = torch.max(qp, axis=1)\n",
    "\n",
    "        # get target return using target network\n",
    "        q_next = self.get_q_next(sn.cuda())\n",
    "        target_return = rn.cuda() + self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(pred_return, target_return)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.network_sync_counter += 1\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference :\n",
    "# https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
    "# https://github.com/mahakal001/reinforcement-learning\n",
    "\n",
    "# Create a DQN Agent\n",
    "\n",
    "input_dim = env.observation_space.shape[0]  # 4 => Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n",
    "output_dim = env.action_space.n # 2 => Push cart to the left, Push cart to the right\n",
    "\n",
    "exp_replay_size = 256 # size ot the replay buffer\n",
    "\n",
    "agent = DQN_Agent(seed=1423, layer_sizes=[input_dim, 64, output_dim], lr=1e-3, sync_freq=5,\n",
    "                  exp_replay_size=exp_replay_size) \n",
    "\n",
    "# learning rate = 1e-3\n",
    "# Deep learning neural networks are trained using the stochastic gradient descent algorithm.\n",
    "# The amount that the weights are updated during training is referred to as the step size \n",
    "# or the “learning rate.”\n",
    "# Smaller learning rates require more training epochs given the smaller changes made \n",
    "# to the weights each update, whereas larger learning rates result in rapid changes \n",
    "# and require fewer training epochs.\n",
    "# A learning rate that is too large can cause the model to converge too quickly \n",
    "# to a suboptimal solution, whereas a learning rate that is too small can cause \n",
    "# the process to get stuck\n",
    "\n",
    "# seed=1423\n",
    "# It will set the seed of the random number generator to a fixed value, \n",
    "# the results will be reproducible.\n",
    "\n",
    "# sync_freq=5\n",
    "# Every 5 counts, the target network (self.target_net) will be synchronized with \n",
    "# the q main network ( self.q_net).\n",
    "\n",
    "# layer_sizes=[input_dim, 64, output_dim] -> [4, 64, 2]\n",
    "# The Q \"main network\" will be like this :\n",
    "# Sequential(\n",
    "#  (0): Linear(in_features=4, out_features=64, bias=True)\n",
    "#  (1): Tanh()\n",
    "#  (2): Linear(in_features=64, out_features=2, bias=True)\n",
    "#  (3): Identity()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference :\n",
    "# https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
    "# https://github.com/mahakal001/reinforcement-learning\n",
    "\n",
    "# Main training loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "# tqdm : pip install tqdm\n",
    "# Usefull to show a progress bar\n",
    "# https://tqdm.github.io/\n",
    "\n",
    "losses_list, reward_list, episode_len_list, epsilon_list = [], [], [], []\n",
    "episodes = 10000\n",
    "epsilon = 1\n",
    "\n",
    "# initiliaze experience replay\n",
    "index = 0\n",
    "for i in range(exp_replay_size):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        A = agent.get_action(obs, env.action_space.n, epsilon=1)\n",
    "        obs_next, reward, done, _ = env.step(A.item())\n",
    "        agent.collect_experience([obs, A.item(), reward, obs_next])\n",
    "        obs = obs_next\n",
    "        index += 1\n",
    "        if index > exp_replay_size:\n",
    "            break\n",
    "\n",
    "index = 128\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs, done, losses, ep_len, rew = env.reset(), False, 0, 0, 0\n",
    "    while not done:\n",
    "        ep_len += 1\n",
    "        A = agent.get_action(obs, env.action_space.n, epsilon)\n",
    "        obs_next, reward, done, _ = env.step(A.item())\n",
    "        agent.collect_experience([obs, A.item(), reward, obs_next])\n",
    "\n",
    "        obs = obs_next\n",
    "        rew += reward\n",
    "        index += 1\n",
    "\n",
    "        if index > 128:\n",
    "            index = 0\n",
    "            for j in range(4):\n",
    "                loss = agent.train(batch_size=16)\n",
    "                losses += loss\n",
    "    if epsilon > 0.05:\n",
    "        epsilon -= (1 / 5000)\n",
    "\n",
    "    losses_list.append(losses / ep_len), reward_list.append(rew)\n",
    "    episode_len_list.append(ep_len), epsilon_list.append(epsilon)\n",
    "\n",
    "print(\"Saving trained model\")\n",
    "agent.save_trained_model(\"cartpole-dqn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference :\n",
    "# https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
    "# https://github.com/mahakal001/reinforcement-learning\n",
    "\n",
    "# Use trained model\n",
    "agent.load_pretrained_model(\"cartpole-dqn.pth\")\n",
    "\n",
    "reward_arr = []\n",
    "for i in tqdm(range(100)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    while not done:\n",
    "        A = agent.get_action(obs, env.action_space.n, epsilon=0)\n",
    "        obs, reward, done, info = env.step(A.item())\n",
    "        rew += reward\n",
    "        sleep(0.01)\n",
    "        env.render()\n",
    "\n",
    "    reward_arr.append(rew)\n",
    "\n",
    "t = np.arange(0, 100, 1)\n",
    "plt.plot(t,reward_arr,'ro')\n",
    "plt.show()\n",
    "\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pytorch",
   "language": "python",
   "name": "ve_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
